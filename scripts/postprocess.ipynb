{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the output of viriation\n",
    "\n",
    "The output of the Viriation program is processed through the following steps:\n",
    "1. Reading in the annotations of the mutations -> verifying/pushing changes to our database\n",
    "2. Reading in user feedback for text chunk data and literature level data -> fine-tuning BERT and LightGBM models\n",
    "3. Saving intermediary states including a) papers that have been screened through the classifier b) papers that have been annotated already c) user feedback from the annotation front-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pickle "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 5 column 118 (char 255)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load mutation data from pokay\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../submodules/pokay/output.json\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     mutations_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n",
      "File \u001b[0;32m~/miniconda3/envs/viriation/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/viriation/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/viriation/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/viriation/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 5 column 118 (char 255)"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "# Load mutation data from pokay\n",
    "with open(\"../submodules/pokay/output.json\") as f:\n",
    "    mutations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "\n",
    "class History:\n",
    "    def __init__(self):\n",
    "        self.dates = IntervalTree() # Interval search tree with dates that have been screened -> (start date, end date)\n",
    "        self.papers = {\n",
    "            'relevant' : set(), # papers that passed screening\n",
    "            'irrelevant' : set() # papers that were screened out + papers users said were irrelevant\n",
    "        }\n",
    "\n",
    "\n",
    "    def checkDateRange(self, date_range):\n",
    "        \"\"\" \n",
    "        Determines whether the given date range overlaps with any intervals in the cache of previous scraped dates\n",
    "        \n",
    "        Parameters: \n",
    "        date_range (tuple): Date range with start date and end date\n",
    "\n",
    "        Returns:\n",
    "        bool: Whether the given date range overlaps with any previous date ranges\n",
    "        \"\"\"\n",
    "\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        # Convert to timestamp (float) since intervaltree works on numeric values\n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        # Query for overlapping intervals in the given range\n",
    "        overlapping_intervals = self.dates.overlap(start_ts, end_ts)\n",
    "\n",
    "        return bool(overlapping_intervals)\n",
    "\n",
    "\n",
    "    def addDateRange(self, date_range):\n",
    "        \"\"\"\n",
    "        Adds new date range into the cache of already scraped dates\n",
    "\n",
    "        Parameters:\n",
    "        date_range (tuple): Date range with start date and end date\n",
    "        \"\"\"\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        self.dates[start_ts:end_ts] = (start_dt, end_dt) # Add date range\n",
    "    \n",
    "\n",
    "    def getNonOverlap(self, date_range):\n",
    "        \"\"\" \n",
    "        Returns all dates within the given date range that are not present in the cache of previous scraped dates\n",
    "        \n",
    "        Parameters: \n",
    "        date_range (tuple): Date range with start date and end date\n",
    "\n",
    "        Returns:\n",
    "        list: list of tuples consisting of date ranges that have not been scraped yet\n",
    "        \"\"\"\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        # Convert to timestamp (float) since intervaltree works on numeric values\n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        # Query for overlapping intervals in the given range\n",
    "        overlapping_intervals = self.dates.overlap(start_ts, end_ts)\n",
    "\n",
    "        if not overlapping_intervals:\n",
    "            return [(start_dt, end_dt)]\n",
    "        \n",
    "        overlapping_intervals = sorted(overlapping_intervals)\n",
    "        non_overlapping_ranges = []\n",
    "        current_start = start_ts\n",
    "\n",
    "        # Iterate over each overlapping interval and calculate gaps\n",
    "        for interval in overlapping_intervals:\n",
    "            if current_start < interval.begin:\n",
    "                # There is a gap between the current start and the beginning of this interval\n",
    "                non_overlapping_ranges.append((current_start, interval.begin))\n",
    "            # Update current start to the end of the current interval\n",
    "            current_start = max(current_start, interval.end)\n",
    "        \n",
    "        # Check if there's a gap after the last interval\n",
    "        if current_start < end_ts:\n",
    "            non_overlapping_ranges.append((current_start, end_ts))\n",
    "        \n",
    "        # Convert timestamps back to datetime\n",
    "        non_overlapping_ranges_dt = [\n",
    "            (datetime.fromtimestamp(start), datetime.fromtimestamp(end))\n",
    "            for start, end in non_overlapping_ranges\n",
    "        ]\n",
    "\n",
    "        return non_overlapping_ranges_dt\n",
    "\n",
    "\n",
    "    def updateTree(self):\n",
    "        \"\"\" \n",
    "        Merges all overlapping date ranges within the current cache of scraped dates\n",
    "        \"\"\"\n",
    "\n",
    "        self.dates.merge_overlaps() # merge together overlapping intervals\n",
    "        \n",
    "\n",
    "    def addPaper(self, paper, relevance):\n",
    "        \"\"\" \n",
    "        Updates history of relevant and irrelevant papers that have been processed through the viriation program thus far\n",
    "        \n",
    "        Parameters: \n",
    "        relevant_papers (str): DOI of paper\n",
    "        relevance (bool): Whether or not the paper is relevant\n",
    "        \"\"\"\n",
    "\n",
    "        if relevance:\n",
    "            self.papers['relevant'].add(paper)\n",
    "        \n",
    "        else:\n",
    "            self.papers['irrelevant'].add(paper)\n",
    "        \n",
    "        \n",
    "\n",
    "    def checkPaper(self, paper):\n",
    "        \"\"\" \n",
    "        Checks whether or not a specific paper has been processed by our program before\n",
    "        \n",
    "        Parameters: \n",
    "        paper (str): DOI of paper\n",
    "\n",
    "        Returns:\n",
    "        bool: Whether the paper has been processed by our program before\n",
    "        \"\"\"\n",
    "\n",
    "        return paper in self.papers['relevant'] or paper in self.papers['irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "# Create intermediaries\n",
    "\n",
    "# Hashtable for managing scraping history\n",
    "# scraped_papers = {\n",
    "#     'relevant': set(), # papers that passed screening\n",
    "#     'irrelevant': set(), # papers that were screened out + papers users said were irrelevant\n",
    "#     'dates': [] # Dates that have been screened -> (start date, end date)\n",
    "# }\n",
    "\n",
    "# scraped_papers.append(('2000-01-01', '2022-09-31'))\n",
    "\n",
    "h = History()\n",
    "h.addDateRange(('2000-01-01','2022-01-01'))\n",
    "\n",
    "with open('../data/database/history.pkl', 'wb') as f:\n",
    "    dill.dump(h, f)\n",
    "\n",
    "# Hashtable for managing retrain data in the self-train feature \n",
    "retrain_data = {\n",
    "    'relevant papers': set(), # Positive examples BERT\n",
    "    'irrelevant papers': set(), # Negative examples BERT\n",
    "    'relevant text': set(), # Positive examples LightGBM\n",
    "    'irrelevant text': set() # Negative examples LightGBM\n",
    "}\n",
    "\n",
    "with open('../data/database/self_train.pkl', 'wb') as f:\n",
    "    pickle.dump(retrain_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge:\n",
      "Start: 2023-03-10 00:00:00, End: 2023-04-12 00:00:00\n",
      "Start: 2023-03-24 00:00:00, End: 2024-09-20 00:00:00\n",
      "Start: 2000-01-01 00:00:00, End: 2022-01-01 00:00:00\n",
      "After merge:\n",
      "Start: 2023-03-10 00:00:00, End: 2024-09-20 00:00:00\n",
      "Start: 2000-01-01 00:00:00, End: 2022-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "h = History()\n",
    "h.addDateRange(('2000-01-01','2022-01-01'))\n",
    "h.addDateRange((\"2023-03-10\", \"2023-04-12\"))\n",
    "h.addDateRange((\"2023-03-24\", \"2024-09-20\"))\n",
    "\n",
    "# Print before merging\n",
    "print(\"Before merge:\")\n",
    "for interval in h.dates:\n",
    "    print(f\"Start: {datetime.fromtimestamp(interval.begin)}, End: {datetime.fromtimestamp(interval.end)}\")\n",
    "\n",
    "h.updateTree()\n",
    "\n",
    "# Print after merging\n",
    "print(\"After merge:\")\n",
    "for interval in h.dates:\n",
    "    print(f\"Start: {datetime.fromtimestamp(interval.begin)}, End: {datetime.fromtimestamp(interval.end)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalTree([Interval(946710000.0, 1641020400.0, (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2022, 1, 1, 0, 0))), Interval(1678431600.0, 1681279200.0, (datetime.datetime(2023, 3, 10, 0, 0), datetime.datetime(2023, 4, 12, 0, 0))), Interval(1678431600.0, 1726812000.0), Interval(1679637600.0, 1726812000.0, (datetime.datetime(2023, 3, 24, 0, 0), datetime.datetime(2024, 9, 20, 0, 0)))])\n",
      "[Interval(1678431600.0, 1726812000.0), Interval(946710000.0, 1641020400.0, (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2022, 1, 1, 0, 0)))]\n"
     ]
    }
   ],
   "source": [
    "h.addDateRange((\"2023-03-10\", \"2023-04-12\"))\n",
    "h.addDateRange((\"2023-03-24\", \"2024-09-20\"))\n",
    "print(h.dates)\n",
    "h.updateTree()\n",
    "intervals = list(h.dates)\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# STEP 1: Reading data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m files \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39m../../data/database/annotations/\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m*/*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m annotations_data \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# STEP 1: Reading data\n",
    "files = Path('../../data/database/annotations/').glob('*/*')\n",
    "\n",
    "annotations_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Read each line (which represents a list in string format)\n",
    "        for line in f:\n",
    "            # Convert the string representation of a list to a Python list\n",
    "            record = ast.literal_eval(line.strip())  # Parse the list\n",
    "            annotations_data.append(record)  # Add it to our data list\n",
    "\n",
    "# Convert the list of lists to a DataFrame\n",
    "# Assuming the data has these columns based on your example: ['Mutation', 'DOI', 'Unknown', 'Attributes', 'Text']\n",
    "annotations_df = pd.DataFrame(annotations_data, columns=['Mutation', 'DOI', 'Location', 'Effect', 'Text'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(annotations_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# STEP 1: Formating data\n",
    "with open(\"../data/database/history.pkl\", \"rb\") as f:\n",
    "    history = dill.load(f)\n",
    "\n",
    "with open(\"../data/database/self_train.pkl\", \"rb\") as f:\n",
    "    self_train = dill.load(f) # keys: relevant papers, irrelevant papers, relevant text, irrelevant text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [DOI, Classification]\n",
      "Index: []\n",
      "                         DOI  Classification\n",
      "0  10.1101/2023.04.17.536926               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# STEP 2: Paper level feedback\n",
    "\n",
    "# IRRELEVANT PAPERS\n",
    "irrelevant_df = pd.DataFrame(\n",
    "    self_train[\"irrelevant papers\"],  # Convert dictionary to list of tuples\n",
    "    columns=['DOI']  # Specify column names\n",
    ")\n",
    "\n",
    "# Set as irrelevant\n",
    "irrelevant_df[\"Classification\"] = 0\n",
    "\n",
    "# Display the DataFrame\n",
    "print(irrelevant_df)\n",
    "\n",
    "# RELEVANT PAPERS\n",
    "relevant_df = pd.DataFrame(\n",
    "    self_train[\"relevant papers\"],\n",
    "    columns=[\"DOI\"]\n",
    ")\n",
    "\n",
    "# Set as relevant\n",
    "relevant_df[\"Classification\"] = 1\n",
    "papers_df = pd.concat([relevant_df,irrelevant_df], ignore_index=True)\n",
    "print(papers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [text, Classification]\n",
      "Index: []\n",
      "                                                text  Classification\n",
      "0  While most studies focus on receptor binding d...               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# STEP 2: Chunk level feedback\n",
    "\n",
    "# IRRELEVANT PAPERS\n",
    "irrelevant_df = pd.DataFrame(\n",
    "    self_train[\"irrelevant text\"],  # Convert dictionary to list of tuples\n",
    "    columns=['text']  # Specify column names\n",
    ")\n",
    "\n",
    "# Set as irrelevant\n",
    "irrelevant_df[\"Classification\"] = 0\n",
    "\n",
    "# Display the DataFrame\n",
    "print(irrelevant_df)\n",
    "\n",
    "# RELEVANT PAPERS\n",
    "relevant_df = pd.DataFrame(\n",
    "    self_train[\"relevant text\"],\n",
    "    columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Set as relevant\n",
    "relevant_df[\"Classification\"] = 1\n",
    "chunks_df = pd.concat([relevant_df,irrelevant_df], ignore_index=True)\n",
    "print(chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Chunk level feedback\n",
    "files = Path('../../data/database/self-train/').glob('*.txt')\n",
    "\n",
    "chunks_data = [] # Negative examples\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Read each line (which represents a list in string format)\n",
    "        for line in f:\n",
    "            # record = ast.literal_eval(line.strip())  # Parse the list\n",
    "            # chunks_data.append(record)  # Add it to our data list\n",
    "            chunks_data.append([line, \"irrelevant\"])  # Add it to our data list\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks_data, columns=[\"Text\", \"Classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
