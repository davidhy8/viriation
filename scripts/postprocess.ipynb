{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the output of viriation\n",
    "\n",
    "The output of the Viriation program is processed through the following steps:\n",
    "1. Reading in the annotations of the mutations -> verifying/pushing changes to our database\n",
    "2. Reading in user feedback for text chunk data and literature level data -> fine-tuning BERT and LightGBM models\n",
    "3. Saving intermediary states including a) papers that have been screened through the classifier b) papers that have been annotated already c) user feedback from the annotation front-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "\n",
    "class History:\n",
    "    def __init__(self):\n",
    "        self.dates = IntervalTree() # Interval search tree with dates that have been screened -> (start date, end date)\n",
    "        self.papers = {\n",
    "            'relevant' : set(), # papers that passed screening\n",
    "            'irrelevant' : set() # papers that were screened out + papers users said were irrelevant\n",
    "        }\n",
    "\n",
    "\n",
    "    def checkDateRange(self, date_range):\n",
    "        \"\"\" \n",
    "        Determines whether the given date range overlaps with any intervals in the cache of previous scraped dates\n",
    "        \n",
    "        Parameters: \n",
    "        date_range (tuple): Date range with start date and end date\n",
    "\n",
    "        Returns:\n",
    "        bool: Whether the given date range overlaps with any previous date ranges\n",
    "        \"\"\"\n",
    "\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        # Convert to timestamp (float) since intervaltree works on numeric values\n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        # Query for overlapping intervals in the given range\n",
    "        overlapping_intervals = self.dates.overlap(start_ts, end_ts)\n",
    "\n",
    "        return bool(overlapping_intervals)\n",
    "\n",
    "\n",
    "    def addDateRange(self, date_range):\n",
    "        \"\"\"\n",
    "        Adds new date range into the cache of already scraped dates\n",
    "\n",
    "        Parameters:\n",
    "        date_range (tuple): Date range with start date and end date\n",
    "        \"\"\"\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        self.dates[start_ts:end_ts] = (start_dt, end_dt) # Add date range\n",
    "    \n",
    "\n",
    "    def getNonOverlap(self, date_range):\n",
    "        \"\"\" \n",
    "        Returns all dates within the given date range that are not present in the cache of previous scraped dates\n",
    "        \n",
    "        Parameters: \n",
    "        date_range (tuple): Date range with start date and end date\n",
    "\n",
    "        Returns:\n",
    "        list: list of tuples consisting of date ranges that have not been scraped yet\n",
    "        \"\"\"\n",
    "        start_dt, end_dt = date_range\n",
    "        start_dt = datetime.strptime(start_dt, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_dt, '%Y-%m-%d')\n",
    "        \n",
    "        # Convert to timestamp (float) since intervaltree works on numeric values\n",
    "        start_ts = start_dt.timestamp()\n",
    "        end_ts = end_dt.timestamp()\n",
    "\n",
    "        # Query for overlapping intervals in the given range\n",
    "        overlapping_intervals = self.dates.overlap(start_ts, end_ts)\n",
    "\n",
    "        if not overlapping_intervals:\n",
    "            return [(start_dt, end_dt)]\n",
    "        \n",
    "        overlapping_intervals = sorted(overlapping_intervals)\n",
    "        non_overlapping_ranges = []\n",
    "        current_start = start_ts\n",
    "\n",
    "        # Iterate over each overlapping interval and calculate gaps\n",
    "        for interval in overlapping_intervals:\n",
    "            if current_start < interval.begin:\n",
    "                # There is a gap between the current start and the beginning of this interval\n",
    "                non_overlapping_ranges.append((current_start, interval.begin))\n",
    "            # Update current start to the end of the current interval\n",
    "            current_start = max(current_start, interval.end)\n",
    "        \n",
    "        # Check if there's a gap after the last interval\n",
    "        if current_start < end_ts:\n",
    "            non_overlapping_ranges.append((current_start, end_ts))\n",
    "        \n",
    "        # Convert timestamps back to datetime\n",
    "        non_overlapping_ranges_dt = [\n",
    "            (datetime.fromtimestamp(start), datetime.fromtimestamp(end))\n",
    "            for start, end in non_overlapping_ranges\n",
    "        ]\n",
    "\n",
    "        return non_overlapping_ranges_dt\n",
    "\n",
    "\n",
    "    def updateTree(self):\n",
    "        \"\"\" \n",
    "        Merges all overlapping date ranges within the current cache of scraped dates\n",
    "        \"\"\"\n",
    "\n",
    "        self.dates.merge_overlaps() # merge together overlapping intervals\n",
    "        \n",
    "\n",
    "    def addPaper(self, paper, relevance):\n",
    "        \"\"\" \n",
    "        Updates history of relevant and irrelevant papers that have been processed through the viriation program thus far\n",
    "        \n",
    "        Parameters: \n",
    "        relevant_papers (str): DOI of paper\n",
    "        relevance (bool): Whether or not the paper is relevant\n",
    "        \"\"\"\n",
    "\n",
    "        if relevance:\n",
    "            self.papers['relevant'].add(paper)\n",
    "        \n",
    "        else:\n",
    "            self.papers['irrelevant'].add(paper)\n",
    "        \n",
    "        \n",
    "\n",
    "    def checkPaper(self, paper):\n",
    "        \"\"\" \n",
    "        Checks whether or not a specific paper has been processed by our program before\n",
    "        \n",
    "        Parameters: \n",
    "        paper (str): DOI of paper\n",
    "\n",
    "        Returns:\n",
    "        bool: Whether the paper has been processed by our program before\n",
    "        \"\"\"\n",
    "\n",
    "        return paper in self.papers['relevant'] or paper in self.papers['irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "# Create intermediaries\n",
    "\n",
    "# Hashtable for managing scraping history\n",
    "# scraped_papers = {\n",
    "#     'relevant': set(), # papers that passed screening\n",
    "#     'irrelevant': set(), # papers that were screened out + papers users said were irrelevant\n",
    "#     'dates': [] # Dates that have been screened -> (start date, end date)\n",
    "# }\n",
    "\n",
    "# scraped_papers.append(('2000-01-01', '2022-09-31'))\n",
    "\n",
    "h = History()\n",
    "h.addDateRange(('2000-01-01','2022-01-01'))\n",
    "\n",
    "with open('../data/database/history.pkl', 'wb') as f:\n",
    "    dill.dump(h, f)\n",
    "\n",
    "# Hashtable for managing retrain data in the self-train feature \n",
    "retrain_data = {\n",
    "    'relevant papers': set(), # Positive examples BERT\n",
    "    'irrelevant papers': set(), # Negative examples BERT\n",
    "    'relevant text': set(), # Positive examples LightGBM\n",
    "    'irrelevant text': set() # Negative examples LightGBM\n",
    "}\n",
    "\n",
    "with open('../data/database/self_train.pkl', 'wb') as f:\n",
    "    pickle.dump(retrain_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge:\n",
      "Start: 2023-03-10 00:00:00, End: 2023-04-12 00:00:00\n",
      "Start: 2023-03-24 00:00:00, End: 2024-09-20 00:00:00\n",
      "Start: 2000-01-01 00:00:00, End: 2022-01-01 00:00:00\n",
      "After merge:\n",
      "Start: 2023-03-10 00:00:00, End: 2024-09-20 00:00:00\n",
      "Start: 2000-01-01 00:00:00, End: 2022-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "h = History()\n",
    "h.addDateRange(('2000-01-01','2022-01-01'))\n",
    "h.addDateRange((\"2023-03-10\", \"2023-04-12\"))\n",
    "h.addDateRange((\"2023-03-24\", \"2024-09-20\"))\n",
    "\n",
    "# Print before merging\n",
    "print(\"Before merge:\")\n",
    "for interval in h.dates:\n",
    "    print(f\"Start: {datetime.fromtimestamp(interval.begin)}, End: {datetime.fromtimestamp(interval.end)}\")\n",
    "\n",
    "h.updateTree()\n",
    "\n",
    "# Print after merging\n",
    "print(\"After merge:\")\n",
    "for interval in h.dates:\n",
    "    print(f\"Start: {datetime.fromtimestamp(interval.begin)}, End: {datetime.fromtimestamp(interval.end)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalTree([Interval(946710000.0, 1641020400.0, (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2022, 1, 1, 0, 0))), Interval(1678431600.0, 1681279200.0, (datetime.datetime(2023, 3, 10, 0, 0), datetime.datetime(2023, 4, 12, 0, 0))), Interval(1678431600.0, 1726812000.0), Interval(1679637600.0, 1726812000.0, (datetime.datetime(2023, 3, 24, 0, 0), datetime.datetime(2024, 9, 20, 0, 0)))])\n",
      "[Interval(1678431600.0, 1726812000.0), Interval(946710000.0, 1641020400.0, (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2022, 1, 1, 0, 0)))]\n"
     ]
    }
   ],
   "source": [
    "h.addDateRange((\"2023-03-10\", \"2023-04-12\"))\n",
    "h.addDateRange((\"2023-03-24\", \"2024-09-20\"))\n",
    "print(h.dates)\n",
    "h.updateTree()\n",
    "intervals = list(h.dates)\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mutation                        DOI Location       Effect  \\\n",
      "0    H655Y  10.1101/2023.04.17.536926     None  [Homoplasy]   \n",
      "1    N679K  10.1101/2023.04.17.536926     None           []   \n",
      "2    P681H  10.1101/2023.04.17.536926     None           []   \n",
      "\n",
      "                                                Text  \n",
      "0  [While most studies focus on receptor binding ...  \n",
      "1  [While most studies focus on receptor binding ...  \n",
      "2  [While most studies focus on receptor binding ...  \n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Reading data\n",
    "files = Path('../../data/database/annotations/').glob('*/*')\n",
    "\n",
    "annotations_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Read each line (which represents a list in string format)\n",
    "        for line in f:\n",
    "            # Convert the string representation of a list to a Python list\n",
    "            record = ast.literal_eval(line.strip())  # Parse the list\n",
    "            annotations_data.append(record)  # Add it to our data list\n",
    "\n",
    "# Convert the list of lists to a DataFrame\n",
    "# Assuming the data has these columns based on your example: ['Mutation', 'DOI', 'Unknown', 'Attributes', 'Text']\n",
    "annotations_df = pd.DataFrame(annotations_data, columns=['Mutation', 'DOI', 'Location', 'Effect', 'Text'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(annotations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Formating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Updating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.1101_2023.07.02.547076': 'irrelevant', '10.1101_2023.04.17.536926': 'relevant'}\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Paper level feedback\n",
    "\n",
    "\n",
    "with open('../../data/database/self-train/irrelevant_papers.pkl', 'rb') as f:\n",
    "    papers = pickle.load(f)\n",
    "\n",
    "# Create DataFrame with three columns\n",
    "irrelevant_df = pd.DataFrame(\n",
    "    list(papers.items()),  # Convert dictionary to list of tuples\n",
    "    columns=['DOI', 'Classification']  # Specify column names\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(irrelevant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Chunk level feedback\n",
    "files = Path('../../data/database/self-train/').glob('*.txt')\n",
    "\n",
    "chunks_data = [] # Negative examples\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Read each line (which represents a list in string format)\n",
    "        for line in f:\n",
    "            # record = ast.literal_eval(line.strip())  # Parse the list\n",
    "            # chunks_data.append(record)  # Add it to our data list\n",
    "            chunks_data.append([line, \"irrelevant\"])  # Add it to our data list\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks_data, columns=[\"Text\", \"Classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
