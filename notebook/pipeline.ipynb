{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608ed739-c748-4282-a2df-7a8af47818bb",
   "metadata": {},
   "source": [
    "# Paper flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe099315-a725-415e-b9fd-0ad9e324e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/david.yang1/.cache/huggingface/'\n",
    "os.environ['HF_HOME'] = '/home/david.yang1/.cache/huggingface/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2daaaf9-b7aa-4b88-8731-117600c28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "# from huggingface_hub import login\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4340054-f128-4268-ac4a-1c06b8ca056f",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438d10-ed93-4005-8110-84f9a48c39fb",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "936dd18c-ad8f-4fa3-8971-cf2ec58a8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(\n",
    "        df['text'],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6df581e6-362f-4698-8022-ce12b4e66de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_preparation(df, val_count=0):\n",
    "    # Balance classes if needed\n",
    "    df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = df[[\"text\", \"label\"]]\n",
    "    \n",
    "    # Split dataset into test & train\n",
    "    df_train = df[val_count:]\n",
    "    df_val = df[:val_count]\n",
    "    \n",
    "    tds = Dataset.from_pandas(df_train)\n",
    "    vds = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    tds = tds.map(tokenize_function, batched=True)\n",
    "    vds = vds.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format of the datasets to include only the required columns\n",
    "    tds = tds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    vds = vds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    \n",
    "    # Define DatasetDict\n",
    "    ds = DatasetDict({\n",
    "        \"train\": tds,\n",
    "        \"validation\": vds\n",
    "    })\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07a8d-89f1-4c65-96ca-f649ecb71544",
   "metadata": {},
   "source": [
    "### Model fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4991a887-017e-42c9-9348-00fb299443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc5b9950-b36c-47f0-bf4e-74cfaf319ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune model\n",
    "def fine_tune_model(ds, model_init, train=False):\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=3,    # number of training epochs\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Create the Trainer and start training\n",
    "    trainer = Trainer(\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        model_init=model_init,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        trainer.train()\n",
    "\n",
    "    if ds[\"validation\"]:\n",
    "        trainer.evaluate()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86482e9f-cee7-421f-b90b-e32dc0be10fb",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19e53981-bcdc-4cec-bbf9-90fac21d6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into <512 token chunks\n",
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, overlap_sentences=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence using BERT Tokenizer\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # Finalize the current chunk if adding this sentence exceed token limit\n",
    "        if current_chunk_len + token_count > max_tokens:\n",
    "            text_chunk = \" \".join(current_chunk)\n",
    "            chunks.append(text_chunk)\n",
    "\n",
    "            # Create the next chunk with overlap\n",
    "            overlap_start = max(0, i-overlap_sentences)\n",
    "            current_chunk = []\n",
    "            for j in range(overlap_start, i):\n",
    "                current_chunk.append(sentences[j])\n",
    "            current_chunk_len = len(current_chunk)\n",
    "\n",
    "        # Add the current sentence tokens to the chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_len += token_count\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        text_chunk = \" \".join(current_chunk)\n",
    "        chunks.append(text_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d9793bb-d2bc-47b7-8a8c-2a5f28a38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label of dataframe\n",
    "def prediction_chunks(df, tokenizer, trainer):\n",
    "    output = pd.DataFrame()\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "        chunks = split_text_into_chunks(text, tokenizer)\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "        # chunks_df[\"label\"] = df[\"label\"][i]\n",
    "        chunks_df[\"position\"] = chunks_df.index\n",
    "        chunks_df[\"paper\"] = i\n",
    "        \n",
    "        t = Dataset.from_pandas(chunks_df)\n",
    "        t = t.map(tokenize_function, batched=True)\n",
    "        ds_t = DatasetDict({\n",
    "            \"test\": t\n",
    "        })\n",
    "\n",
    "        pred = trainer.predict(ds_t[\"test\"])\n",
    "        chunks_df[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "\n",
    "        # convert logit score to torch array\n",
    "        torch_logits = torch.from_numpy(pred.predictions)\n",
    "\n",
    "        # get probabilities using softmax from logit score and convert it to numpy array\n",
    "        probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()\n",
    "\n",
    "        chunks_df[\"probability\"] = probabilities_scores.max(-1)\n",
    "\n",
    "        # save into output\n",
    "        output = pd.concat([output, chunks_df], ignore_index=True)\n",
    "        \n",
    "    return output, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac674e-0e4b-4b6f-8f51-3112c3a835de",
   "metadata": {},
   "source": [
    "### Dataset to validate chunk prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22be07ee-b6e7-4ddf-b3f4-0025c5b3c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656b4e64c8e84b028d58ad2bbed0cec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load prediction chunks\n",
    "pred_chunks_0 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/0_chunks_labelled.csv\")\n",
    "pred_chunks_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/1_chunks_labelled.csv\")\n",
    "pred_chunks_2 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/2_chunks_labelled.csv\")\n",
    "pred_chunks_3 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/3_chunks_labelled.csv\")\n",
    "pred_chunks_4 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/4_chunks_labelled.csv\")\n",
    "pred_chunks_5 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/5_chunks_labelled.csv\")\n",
    "pred_chunks_6 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/6_chunks_labelled.csv\")\n",
    "pred_chunks_7 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/7_chunks_labelled.csv\")\n",
    "pred_chunks_8 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/8_chunks_labelled.csv\")\n",
    "pred_chunks_9 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/9_chunks_labelled.csv\")\n",
    "pred_chunks_10 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/10_chunks_labelled.csv\")\n",
    "pred_chunks_11 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/11_chunks_labelled.csv\")\n",
    "\n",
    "\n",
    "# Concatenate data\n",
    "df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2, pred_chunks_3, pred_chunks_4, pred_chunks_5, pred_chunks_6, pred_chunks_7, \n",
    "                     pred_chunks_8, pred_chunks_9, pred_chunks_10, pred_chunks_11])\n",
    "\n",
    "# Load dataframe as dataset\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenize test dataset\n",
    "test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format of the datasets to include only the required columns\n",
    "test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds_test = DatasetDict({\n",
    "    \"test\": test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "572b20e2-b847-46fc-a6a5-1e2f2035bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer):\n",
    "    # Test performance of the model on labeled chunks\n",
    "    pred = trainer.predict(ds_test[\"test\"])\n",
    "    \n",
    "    df_test[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "    \n",
    "    metrics = compute_metrics(pred)\n",
    "\n",
    "    # df_test.to_csv(\"df_test_chunks.csv\")\n",
    "\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed027-9924-4660-8ad7-c3fa05bc1085",
   "metadata": {},
   "source": [
    "## Chunk labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3220db65-f793-4cbc-899d-22d20f10d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469f95226ed043128a6e00cecbc1093f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989abb250d5e4b519301c251b11305f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.68, 'f1': 0.6875000000000001, 'precision': 0.5301204819277109, 'recall': 0.9777777777777777}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/pipeline_data/paper_flagging_data/bert_dataset.csv')\n",
    "ds = ds_preparation(df, val_count=128)\n",
    "\n",
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "# Check performance\n",
    "metrics = validate_model(trainer)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82585ea9-795d-4edb-9bfb-d3c72298def9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3173f5cf921d496f84a65a69e2fb90e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7900115f036748db888b770c2a88575e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87aa04bc8c94258a5a5044176851201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fcdf29fd26401390a842c08a5867cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f649ca81914d08a46630da9a14ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab9c9ec212a4084b7cedbcb60ec1fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cfe60c49c248059d7cffd45ba9b8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b73ec49e1934f8cb5a79cbe5db1a90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c302c764e64c1da11889a5c38c7b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49854fa681af42a6bbcbcd3e664f0a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load new papers\n",
    "data = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/new_papers_dataset.csv\")\n",
    "chunked_data_df, chunked_data_pred = prediction_chunks(data[:10], tokenizer, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2276a-109c-44b1-af3f-ec5354bb1262",
   "metadata": {},
   "source": [
    "## Paper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "810b9d18-9cf7-42b8-b8b5-478d4f08960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9eef3aa1-50ac-4e2f-af12-cbd792635a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightbgm model\n",
    "bst = lgb.Booster(model_file='../models/lightgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d16c3b5a-0c20-4cb8-8b38-3d604c9a5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format lightGBM data \n",
    "data = chunked_data_df\n",
    "\n",
    "# Format data for lightGBM\n",
    "grouped = data.groupby('paper')\n",
    "\n",
    "# Maximum number of data points in any group\n",
    "max_len = 133\n",
    "# max_len = max(grouped.size())\n",
    "# print(max_len)\n",
    "\n",
    "# Create DataFrame with appropriate number of columns\n",
    "columns = [f'prediction_{i}' for i in range(max_len)]\n",
    "columns.append(\"paper\")\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in grouped:\n",
    "    predictions = group[\"prediction\"].values.astype(float)\n",
    "    entry = np.pad(predictions, (0, max_len - len(predictions)), constant_values=np.nan)\n",
    "    # entry = np.pad(predictions, (0, 133 - len(predictions)), constant_values=np.nan)\n",
    "    entry = np.append(entry, name)\n",
    "    df.loc[name] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8de0e9d7-a3a3-4a00-9867-4da224fd33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(df.drop(columns=\"paper\"), num_iteration=bst.best_iteration)\n",
    "pred = np.where(predictions < 0.5, 0, 1)\n",
    "df[\"prediction\"] = pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04401b9e-6e53-4280-9a42-e6f51c9fe567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper'] = df['paper'].astype(int)\n",
    "flagged_papers = df[df['prediction'] == 1]['paper']\n",
    "\n",
    "relevant_papers = flagged_papers.tolist()\n",
    "flagged = chunked_data_df[chunked_data_df[\"paper\"].isin(relevant_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5137b2e0-db2f-4e72-8865-677d01437404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapid detection and tracking of Omicron variant of SARS-CoV-2 using CRISPR-Cas12a-based assayBackgroundThe newly emerged SARS-CoV-2 variant of concern (VOC) Omicron is spreading quickly worldwide, which manifests an urgent need of simple and rapid assay to detect and diagnose Omicron infection and track its spread.MethodsTo design allele-specific CRISPR RNAs (crRNAs) targeting the signature mutations in the spike protein of Omicron variant, and to develop a CRISPR-Cas12a-based assay to specifically detect Omicron variant.ResultsOur system showed a low limit of detection of 2 copies per reaction for the plasmid DNA of Omicron variant, and could readily detect Omicron variant in 5 laboratory-confirmed clinical samples and distinguish them from 57 SARS-CoV-2 positive clinical samples (4 virus isolates and 53 oropharyngeal swab specimens) infected with wild-type (N = 8) and the variants of Alpha (N = 17), Beta (N = 17) and Delta (N = 15). The testing results could be measured by fluorescent detector or judged by naked eyes. In addition, no cross-reaction was observed when detecting 16 clinical samples infected with 9 common respiratory pathogens.ConclusionsThe rapid assay could be easily set up in laboratories already conducting SARS-CoV-2 nucleic acid amplification tests and implemented routinely in resource-limited settings to monitor and track the spread of Omicron variant.IntroductionContinuing spread and evolution of SARS-CoV-2 have resulted in the emergence of various variants that have infected and killed millions of people. The newly emerged fifth variant of concern (VOC) Omicron was firstly reported in South Africa on November 24, 2021 and has been detected in many countries. Omicron variant contains more than 32 amino acid mutations in the spike protein, including multiple vital amino acid mutations (K417N, T478K, E484A, N501Y, and D614G) that have been already detected in other VOCs of SARS-CoV-2 and proved to be associated with enhanced transmissibility, virulence, and greater resistance to the immune protection induced by COVID-19 vaccines. The new features of Omicron manifested the importance of tracking its spread.Reverse transcription polymerase chain reaction (RT-PCR) has been widely used for diagnosing SARS-CoV-2 infection and genotyping SARS-CoV-2 variants.\n"
     ]
    }
   ],
   "source": [
    "print(flagged[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c48393b1-555a-4e82-a00d-f9d81112f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapid detection and tracking of Omicron variant of SARS-CoV-2 using CRISPR-Cas12a-based assayBackgroundThe newly emerged SARS-CoV-2 variant of concern (VOC) Omicron is spreading quickly worldwide, which manifests an urgent need of simple and rapid assay to detect and diagnose Omicron infection and track its spread.MethodsTo design allele-specific CRISPR RNAs (crRNAs) targeting the signature mutations in the spike protein of Omicron variant, and to develop a CRISPR-Cas12a-based assay to specifically detect Omicron variant.ResultsOur system showed a low limit of detection of 2 copies per reaction for the plasmid DNA of Omicron variant, and could readily detect Omicron variant in 5 laboratory-confirmed clinical samples and distinguish them from 57 SARS-CoV-2 positive clinical samples (4 virus isolates and 53 oropharyngeal swab specimens) infected with wild-type (N = 8) and the variants of Alpha (N = 17), Beta (N = 17) and Delta (N = 15).The testing results could be measured by fluorescent detector or judged by naked eyes.In addition, no cross-reaction was observed when detecting 16 clinical samples infected with 9 common respiratory pathogens.ConclusionsThe rapid assay could be easily set up in laboratories already conducting SARS-CoV-2 nucleic acid amplification tests and implemented routinely in resource-limited settings to monitor and track the spread of Omicron variant.IntroductionContinuing spread and evolution of SARS-CoV-2 have resulted in the emergence of various variants that have infected and killed millions of people.The newly emerged fifth variant of concern (VOC) Omicron was firstly reported in South Africa on November 24, 2021 and has been detected in many countries.Omicron variant contains more than 32 amino acid mutations in the spike protein, including multiple vital amino acid mutations (K417N, T478K, E484A, N501Y, and D614G) that have been already detected in other VOCs of SARS-CoV-2 and proved to be associated with enhanced transmissibility, virulence, and greater resistance to the immune protection induced by COVID-19 vaccines.The new features of Omicron manifested the importance of tracking its spread.Reverse transcription polymerase chain reaction (RT-PCR) has been widely used for diagnosing SARS-CoV-2 infection and genotyping SARS-CoV-2 variants.\n"
     ]
    }
   ],
   "source": [
    "print(flagged[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2aaf18-50c2-443f-852c-e3dc4da56320",
   "metadata": {},
   "source": [
    "# NER with BERN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ea1965d-b4ca-453e-b518-b0b2575fc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "079dd373-b6cb-47a6-b06f-ab47e8e0b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()\n",
    "\n",
    "port = \"http://172.19.8.251:8888/plain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "541acd59-63ce-4169-a1ef-283eeea26d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_papers = flagged.groupby('paper')\n",
    "\n",
    "for name, group in grouped_papers:\n",
    "    NER_list = list()\n",
    "    for text in group[\"text\"]:\n",
    "        NER = query_plain(text, url = port)\n",
    "        NER_list.append(NER)\n",
    "    file_name = \"../data/pipeline_data/NER/\" + str(name) + \"_paper.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(NER_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e36a9-c8db-4322-a931-bcda88fee524",
   "metadata": {},
   "source": [
    "# Figure out what each mutation does\n",
    "https://www.reddit.com/r/MachineLearning/comments/o0kixr/improving_bart_text_summarization_by_providing/\n",
    "\n",
    "https://peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9954705-8052-41e1-8ab4-350b56c572d4",
   "metadata": {},
   "source": [
    "# Description of mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8c55a46-8520-4419-aeaf-247edbfa0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e71d36e4-b0db-40b0-ba1f-b8a8b324e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150f0b9-a402-40ea-b1c3-82849e86d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"/home/david.yang1/autolit/viriation/data/pipeline_data/NER\").glob(\"*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acaa88-5efa-4de9-93fa-d62247c704c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutations(files):\n",
    "    for file in files:\n",
    "        # Initialize output dictionary\n",
    "        output = defaultdict(list)\n",
    "        \n",
    "        with open(file, 'rb') as f:\n",
    "            # Load NERs\n",
    "            ner = pickle.load(f) \n",
    "\n",
    "            for ner_chunk in ner:\n",
    "                text = ner_chunk['text']\n",
    "\n",
    "                # Process text chunks for increased readability\n",
    "                chunk = nlp(text)\n",
    "\n",
    "                # Split text into sentences\n",
    "                sentences = [sent.text for sent in chunk.sents]\n",
    "                annotations = ner_chunk['annotations']\n",
    "                \n",
    "                for annotation in annotations:\n",
    "                    if annotation['obj'] == 'mutation':\n",
    "                        mutation = annotation[\"mention\"]\n",
    "                        for count, sent in enumerate(sentences):\n",
    "                            if mutation in sent:\n",
    "                                context = []\n",
    "\n",
    "                                # Save sentence before and after mutation\n",
    "                                if count != 0:\n",
    "                                    context.append(sentences[count-1])\n",
    "    \n",
    "                                context.append(sent)\n",
    "\n",
    "                                if count != (len(sentences)-1):\n",
    "                                    context.append(sentences[count+1])\n",
    "    \n",
    "                                context = \" \".join(context)\n",
    "    \n",
    "                                output[mutation].append(context) \n",
    "        \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
