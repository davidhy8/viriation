{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe099315-a725-415e-b9fd-0ad9e324e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/david.yang1/.cache/huggingface/'\n",
    "os.environ['HF_HOME'] = '/home/david.yang1/.cache/huggingface/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2daaaf9-b7aa-4b88-8731-117600c28586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-07-18 09:51:03.705982: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 09:51:03.749374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 09:51:03.749412: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 09:51:03.749448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 09:51:03.758072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 09:51:04.632573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "# from huggingface_hub import login\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "# from ray.tune.search.hyperopt import HyperOptSearch\n",
    "# from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438d10-ed93-4005-8110-84f9a48c39fb",
   "metadata": {},
   "source": [
    "# Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936dd18c-ad8f-4fa3-8971-cf2ec58a8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(\n",
    "        df['text'],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df581e6-362f-4698-8022-ce12b4e66de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_preparation(df, val_count=0):\n",
    "    # Balance classes if needed\n",
    "    df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = df[[\"text\", \"label\"]]\n",
    "    \n",
    "    # Split dataset into test & train\n",
    "    df_train = df[val_count:]\n",
    "    df_val = df[:val_count]\n",
    "    \n",
    "    tds = Dataset.from_pandas(df_train)\n",
    "    vds = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    tds = tds.map(tokenize_function, batched=True)\n",
    "    vds = vds.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format of the datasets to include only the required columns\n",
    "    tds = tds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    vds = vds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    \n",
    "    # Define DatasetDict\n",
    "    ds = DatasetDict({\n",
    "        \"train\": tds,\n",
    "        \"validation\": vds\n",
    "    })\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07a8d-89f1-4c65-96ca-f649ecb71544",
   "metadata": {},
   "source": [
    "# Model fine tuning\n",
    "Parameter tuning: \n",
    "https://kaitchup.substack.com/p/a-guide-on-hyperparameters-and-training\n",
    "https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "https://huggingface.co/blog/ray-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4991a887-017e-42c9-9348-00fb299443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38cf31b-9a57-43aa-a71a-a1026c6ddeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = True\n",
    "\n",
    "# Load initial fine-tuned model\n",
    "if checkpoint:\n",
    "    def model_init():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\"./chunks-pubmed-bert\", num_labels=2)\n",
    "else:\n",
    "    def model_init():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\"NeuML/pubmedbert-base-embeddings\", num_labels=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5b9950-b36c-47f0-bf4e-74cfaf319ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune model\n",
    "def fine_tune_model(ds, model_init, train=False):\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=3,    # number of training epochs\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Create the Trainer and start training\n",
    "    trainer = Trainer(\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        model_init=model_init,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        trainer.train()\n",
    "\n",
    "    if ds[\"validation\"]:\n",
    "        trainer.evaluate()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86482e9f-cee7-421f-b90b-e32dc0be10fb",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e53981-bcdc-4cec-bbf9-90fac21d6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into <512 token chunks\n",
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, overlap_sentences=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence using BERT Tokenizer\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # Finalize the current chunk if adding this sentence exceed token limit\n",
    "        if current_chunk_len + token_count > max_tokens:\n",
    "            text_chunk = \"\".join(current_chunk)\n",
    "            chunks.append(text_chunk)\n",
    "\n",
    "            # Create the next chunk with overlap\n",
    "            overlap_start = max(0, i-overlap_sentences)\n",
    "            current_chunk = []\n",
    "            for j in range(overlap_start, i):\n",
    "                current_chunk.append(sentences[j])\n",
    "            current_chunk_len = len(current_chunk)\n",
    "\n",
    "        # Add the current sentence tokens to the chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_len += token_count\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        text_chunk = \"\".join(current_chunk)\n",
    "        chunks.append(text_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9793bb-d2bc-47b7-8a8c-2a5f28a38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label of dataframe\n",
    "def prediction_chunks(df, tokenizer, trainer):\n",
    "    output = pd.DataFrame()\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "        chunks = split_text_into_chunks(text, tokenizer)\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "        # chunks_df[\"label\"] = df[\"label\"][i]\n",
    "        chunks_df[\"position\"] = chunks_df.index\n",
    "        chunks_df[\"paper\"] = i\n",
    "        \n",
    "        t = Dataset.from_pandas(chunks_df)\n",
    "        t = t.map(tokenize_function, batched=True)\n",
    "        ds_t = DatasetDict({\n",
    "            \"test\": t\n",
    "        })\n",
    "\n",
    "        pred = trainer.predict(ds_t[\"test\"])\n",
    "        chunks_df[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "\n",
    "        # convert logit score to torch array\n",
    "        torch_logits = torch.from_numpy(pred.predictions)\n",
    "\n",
    "        # get probabilities using softmax from logit score and convert it to numpy array\n",
    "        probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()\n",
    "\n",
    "        chunks_df[\"probability\"] = probabilities_scores.max(-1)\n",
    "\n",
    "        # save into output\n",
    "        output = pd.concat([output, chunks_df], ignore_index=True)\n",
    "        \n",
    "    return output, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac674e-0e4b-4b6f-8f51-3112c3a835de",
   "metadata": {},
   "source": [
    "# Dataset to validate chunk prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22be07ee-b6e7-4ddf-b3f4-0025c5b3c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8641f7934b4c33bae9088d133ca25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load prediction chunks\n",
    "pred_chunks_0 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/0_chunks_labelled.csv\")\n",
    "pred_chunks_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/1_chunks_labelled.csv\")\n",
    "pred_chunks_2 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/2_chunks_labelled.csv\")\n",
    "pred_chunks_3 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/3_chunks_labelled.csv\")\n",
    "pred_chunks_4 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/4_chunks_labelled.csv\")\n",
    "pred_chunks_5 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/5_chunks_labelled.csv\")\n",
    "pred_chunks_6 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/6_chunks_labelled.csv\")\n",
    "pred_chunks_7 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/7_chunks_labelled.csv\")\n",
    "pred_chunks_8 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/8_chunks_labelled.csv\")\n",
    "pred_chunks_9 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/9_chunks_labelled.csv\")\n",
    "pred_chunks_10 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/10_chunks_labelled.csv\")\n",
    "pred_chunks_11 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/11_chunks_labelled.csv\")\n",
    "\n",
    "\n",
    "# Concatenate data\n",
    "df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2, pred_chunks_3, pred_chunks_4, pred_chunks_5, pred_chunks_6, pred_chunks_7, \n",
    "                     pred_chunks_8, pred_chunks_9, pred_chunks_10, pred_chunks_11])\n",
    "\n",
    "# Load dataframe as dataset\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenize test dataset\n",
    "test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format of the datasets to include only the required columns\n",
    "test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds_test = DatasetDict({\n",
    "    \"test\": test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d564af-951d-42d3-ab9d-2e15f8cf11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# pred = trainer.predict(ds_test[\"test\"])\n",
    "\n",
    "# results = perplexity.compute(predictions=predictions, model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35f5b7e-001a-4467-87ff-a6a16a96d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer):\n",
    "    # Test performance of the model on labeled chunks\n",
    "    pred = trainer.predict(ds_test[\"test\"])\n",
    "    \n",
    "    df_test[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "    \n",
    "    metrics = compute_metrics(pred)\n",
    "\n",
    "    # df_test.to_csv(\"df_test_chunks.csv\")\n",
    "\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985f351d-4430-4f8b-85d1-260897853ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities_scores.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a151b1e-16dd-498e-8fc4-21e82ff8d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert logit score to torch array\n",
    "# torch_logits = torch.from_numpy(pred.predictions)\n",
    "\n",
    "# # get probabilities using softmax from logit score and convert it to numpy array\n",
    "# probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac5b060-c55e-4d31-ab00-eaeb3d5d182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961baf27-6ec0-49a3-8865-e2955756f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2abee-9cea-433a-b50f-fc6c9b437db8",
   "metadata": {},
   "source": [
    "# View model prediction on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c38ecea-b0a0-4b6c-b7ee-f6d344eac8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = split_text_into_chunks(df[\"text\"][0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd69412b-d46e-41ee-bcd7-ef8875903edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c7c458-e727-4ee2-bb5b-6d06d0f56401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_df = pd.DataFrame(chunks, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79dc0bed-b71c-4e8d-af6f-b5a8d8fccad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(chunk_df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a913b82e-a9f4-4294-947c-0dae659529ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_df[\"label\"] = df[\"label\"][0]\n",
    "# chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee111fd-8d22-459d-895e-e56b7c7415a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = Dataset.from_pandas(chunk_df)\n",
    "# t = t.map(tokenize_function, batched=True)\n",
    "# # t = t.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# # Define DatasetDict\n",
    "# ds_t = DatasetDict({\n",
    "#     \"test\": t\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18fdbc2f-4ef0-466a-80bc-e146ac70148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = trainer.predict(ds_t[\"test\"])\n",
    "# pred.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc6c0f-2ab9-41d9-a839-c48110d614d0",
   "metadata": {},
   "source": [
    "data = prediction_chunks(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62387710-c022-4b50-93e3-9b029f1595cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"chunk_labelled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed027-9924-4660-8ad7-c3fa05bc1085",
   "metadata": {},
   "source": [
    "# Perform prediction and retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8e91e-94ca-4fd0-a447-139ffa0ef7c0",
   "metadata": {},
   "source": [
    "**Documentation**: June 26 - Transfer learning of PubmedBERT with Pokay dataset at ./chunks-pubmed-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2987bc2d-2ef7-48ac-a56c-262491053963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49673a8eaf6e4126806c3f4b7a9d446d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9cf597422431a8dd4ffa40de07bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NeuML/pubmedbert-base-embeddings and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NeuML/pubmedbert-base-embeddings and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 14:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.656, 'f1': 0.6666666666666665, 'precision': 0.5119047619047619, 'recall': 0.9555555555555556}\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"NeuML/pubmedbert-base-embeddings\", num_labels=2)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/pipeline_data/paper_flagging_data/bert_dataset.csv')\n",
    "ds = ds_preparation(df, val_count=128)\n",
    "\n",
    "# Train model\n",
    "trainer = fine_tune_model(ds, model_init, train=True)\n",
    "\n",
    "# Checkpoint model\n",
    "trainer.save_model(\"../models/chunks-pubmed-bert\")\n",
    "\n",
    "# Validate current model\n",
    "metrics = validate_model(trainer)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5eb82b8-c688-441f-943e-c5b426ce1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f3765-1b52-4889-a3b1-12325d019e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "# Check performance\n",
    "metrics = validate_model(trainer)\n",
    "print(metrics)\n",
    "# Save predictions \n",
    "df_test.to_csv(\"test_predictions_model_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed4ad-62a9-4ae6-acc9-f40710f3e08b",
   "metadata": {},
   "source": [
    "## Load retrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9f9ae-65b6-4119-8c2d-cfc6e10d7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retrain data\n",
    "retrain_df = pd.read_csv('chunks_dataset.csv')\n",
    "# print(retrain_df)\n",
    "retrain_df = retrain_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31875519-8495-4f54-99a1-118d0645ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model\n",
    "# def model_init():\n",
    "#     return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert\", num_labels=2)\n",
    "\n",
    "# # Load data\n",
    "# df = pd.read_csv('bert_dataset.csv')\n",
    "# ds = ds_preparation(df)\n",
    "\n",
    "# # Train model\n",
    "# trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "retrain_df_1 = retrain_df[:100]\n",
    "retrain_df_1, _ = prediction_chunks(retrain_df_1, tokenizer, trainer)\n",
    "# retrain_df_1\n",
    "\n",
    "retrain_df_1.describe()\n",
    "\n",
    "# Rename column\n",
    "retrain_df_1 = retrain_df_1.rename(columns={'prediction': 'label'})\n",
    "\n",
    "print(retrain_df_1[retrain_df_1[\"probability\"] < 0.7].count())\n",
    "\n",
    "# Filter for predictions by the decision boundary\n",
    "retrain_df_1 = retrain_df_1[retrain_df_1[\"probability\"] < 0.7]\n",
    "\n",
    "# Balance out 0 and 1 labels \n",
    "retrain_df_1 = retrain_df_1.groupby('label').sample(n=min(retrain_df_1['label'].value_counts()), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f984cf-f222-4ae1-8ba3-852a1fda9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee379ee-9a1e-448c-bcb8-d98018d4846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset locally to verify validity of predictions\n",
    "retrain_df_1.to_csv(\"retrain_df_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6b2ab-6119-48d3-a6ab-af4a0364a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modified dataset\n",
    "retrain_df_1 = pd.read_csv(\"../data/processed/train_data/retrain_df_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6ecbe-b137-41dd-ba34-8733bf7df42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove german paper\n",
    "retrain_df_1 = retrain_df_1.drop(retrain_df_1[retrain_df_1['paper'] == 43].index)\n",
    "\n",
    "# balance out classes\n",
    "retrain_df_1 = retrain_df_1.groupby('label').sample(n=min(retrain_df_1['label'].value_counts()), random_state=42)\n",
    "\n",
    "# Drop unneccessary columns\n",
    "retrain_df = retrain_df_1.drop(['position', 'paper', 'probability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa3213-7baa-46fa-8de2-527d6d9f351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d1f68-d8d5-471d-b5d0-09f0d0841b88",
   "metadata": {},
   "source": [
    "**Documentation**: June 28 - Retrain of model #1 saved at ./chunks-pubmed-bert-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e54ed-d24b-4faa-b435-3f36aac3f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "retrain_ds = ds_preparation(retrain_df)\n",
    "\n",
    "# Load model \n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert\", num_labels=2)\n",
    "\n",
    "# Train model\n",
    "trainer = fine_tune_model(retrain_ds, model_init, train=True)\n",
    "\n",
    "# Checkpoint model\n",
    "trainer.save_model(\"./chunks-pubmed-bert-v2\")\n",
    "\n",
    "# Validate current model\n",
    "metrics = validate_model(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9baa55b-212b-4a8d-aa83-e6da7f5d087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220db65-f793-4cbc-899d-22d20f10d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "# Check performance\n",
    "metrics = validate_model(trainer)\n",
    "print(metrics)\n",
    "\n",
    "# Save predictions \n",
    "df_test.to_csv(\"test_predictions_model_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054dd616-10db-4bc6-8f78-9e0906c99dc7",
   "metadata": {},
   "source": [
    "# Create dataset for chunking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a11a3-f35f-448d-ae54-87e69b08eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_data = pd.read_csv(\"bert_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f676d-2c6c-4782-aeb8-d85be15d55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2dd5b-a58d-4839-92e0-990df48916d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = prediction_chunks(full_text_data, tokenizer, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f141dcc-558f-4835-be41-4a92a54eefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data_df, chunked_data_pred = chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefc7a5-a677-494a-b564-82eda05b034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data_df.to_csv(\"../data/pipeline_data/paper_flagging_data/lightgbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf96ded-b514-419e-a737-ad6e1019822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512dccb-5f64-45d5-b704-31a32e2b6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new papers\n",
    "data = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/new_papers_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d71c3e-f187-4aa6-8451-b6c61ba5a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e57b5c-6842-4fde-b5f0-18c7abf05524",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data_df, chunked_data_pred = prediction_chunks(data[:10], tokenizer, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ca51e-1b0d-4587-8712-dbf1206b7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f23213-a56f-4e9d-9c17-f4a9b0aad494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightbgm model\n",
    "bst = lgb.Booster(model_file='../models/lightbgm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea5b11-7acc-43ec-aeed-02323f1fe44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = chunked_data_df\n",
    "\n",
    "# Format data for lightBGM\n",
    "grouped = data.groupby('paper')\n",
    "\n",
    "# Maximum number of data points in any group\n",
    "# max_len = max(grouped.size())\n",
    "max_len = 133\n",
    "print(max_len)\n",
    "\n",
    "# Create DataFrame with appropriate number of columns\n",
    "columns = [f'prediction_{i}' for i in range(max_len)]\n",
    "columns.append(\"paper\")\n",
    "print(columns)\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in grouped:\n",
    "    predictions = group[\"prediction\"].values.astype(float)\n",
    "    entry = np.pad(predictions, (0, max_len - len(predictions)), constant_values=np.nan)\n",
    "    # entry = np.pad(predictions, (0, 133 - len(predictions)), constant_values=np.nan)\n",
    "    entry = np.append(entry, name)\n",
    "    df.loc[name] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830dd80-fe36-492f-b1a7-69383afad1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(df.drop(columns=\"paper\"), num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d986863-f968-4ea1-b0a4-713433c652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(predictions < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b6c17-3494-45ff-bb7f-e31d252823da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d3aee-044c-4a33-a699-8a3be2a82efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c48e77-bede-4340-a712-7726691b9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper'] = df['paper'].astype(int)\n",
    "flagged_papers = df[df['prediction'] == 1]['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39e81d-8c68-4faf-a574-a03063d49721",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = flagged_papers.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835af067-9a2c-45ff-a270-086914e8c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged = chunked_data_df[chunked_data_df[\"paper\"].isin(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581823eb-2565-41d6-964d-6f90523bf0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged[\"text\"][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dd373-b6cb-47a6-b06f-ab47e8e0b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()\n",
    "\n",
    "port = \"http://172.19.5.205:8888/plain\"\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     text = \"Autophagy maintains tumour growth through circulating arginine.\"\n",
    "#     print(query_plain(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541acd59-63ce-4169-a1ef-283eeea26d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_papers = flagged.groupby('paper')\n",
    "\n",
    "for name, group in grouped_papers:\n",
    "    # print(group[\"text\"])\n",
    "    for text in group[\"text\"]:\n",
    "        # print((query_plain(text, url = \"http://bern2.korea.ac.kr/plain\")))\n",
    "        print((query_plain(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebacf1-5feb-4022-9ee5-179ec1725bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f177d-5ccc-44fe-b29a-a81300b04782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
