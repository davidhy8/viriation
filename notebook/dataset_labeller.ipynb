{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b119b-82f6-4297-be47-b71576721124",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset labeller\n",
    "\n",
    "This tool helps the process of annotating chunk data (512-token size) and sentences within a paper\n",
    "\n",
    "## Labelling Guidelines\n",
    "Only label 1 = Yes if it:\n",
    "- Includes specific terms like \"Delta variant\" or any specific variant name\n",
    "- Includes terms like \"mutation\", \"viral variant\", \"strain\", \"variant of concern\" and \"genetic variant\"\n",
    "- Describes characteristics, behaviours, or impacts of the variants, even if not named explicitly\n",
    "- Compares different variants\n",
    "- Discusses genetic mutations or sequences related to viral variants\n",
    "- Discusses spread, transmission rates or infection rates associated with specific variants\n",
    "- Discusses how variants affect vaccine efficacy\n",
    "- Describes changes in symptom severity due to variants\n",
    "- Discusses health outcomes associated with variants\n",
    "- Discusses public health measures or responses tailored to specific variants\n",
    "- Discusses time and place of variant emergence and spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5941f9-edce-4268-9430-cef1afc136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffa4b57-44ab-40c7-a000-13c98c9d588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each text in dataframe\n",
    "def get_label(df, column):\n",
    "    label_input=[]\n",
    "    for i in df[column]:\n",
    "        print(i)\n",
    "        \n",
    "        while True:\n",
    "            label = int(input('Does this relate to viral variants? (0 = No, 1 = Yes)'))\n",
    "            if label == 0 or label == 1:\n",
    "                break\n",
    "            \n",
    "        label_input.append(label)\n",
    "        print(\" \")\n",
    "    df['label'] = label_input\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e347e-17e0-425b-aad8-f377b7b8bc08",
   "metadata": {},
   "source": [
    "# Labelling 512-token chunks\n",
    "Label all chunks in dataset and save progress every 10 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bddf4-91e0-4c38-a958-51da03c47a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL CHUNKS\n",
    "# Obtain list of sentences \n",
    "df = pd.read_csv(\"chunks_dataset.csv\")\n",
    "# text = df.sample(n=3)\n",
    "\n",
    "# Split dataframe into smaller dataframes to work with\n",
    "df_split = np.array_split(df,30)\n",
    "\n",
    "# Instantiate dataframe\n",
    "result = pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "\n",
    "# Iterate through each split\n",
    "for i, split in enumerate(df_split):\n",
    "    # if i < 3:\n",
    "    #     continue\n",
    "    print(\"Current split \" + str(i))\n",
    "    labelled_chunks = get_label(split, 'text')\n",
    "    filename = str(i) + \"_chunks_labelled.csv\"\n",
    "    labelled_chunks.to_csv(filename)\n",
    "    # labelled_chunks[\"position\"] = i\n",
    "    labelled_chunks.to_csv(filename)\n",
    "    print(\"Finished split \" + str(i))\n",
    "    print(\"============================================================\")\n",
    "    # Combine labelled_split with result\n",
    "    result = pd.concat([result, labelled_chunks])\n",
    "    \n",
    "result.to_csv(\"labelled_chunks.csv\")\n",
    "# print(\"Finished labelling \" + str(text_index) + \" papers.\")\n",
    "# print(labelled_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a57db-9bd6-46f3-b3d7-36453bff2a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Labelling sentences within paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d8130-fb01-4f66-a1c7-f18a81316c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data and create example text\n",
    "# df = pd.read_csv('bn_pub_dataset_3.csv')\n",
    "\n",
    "# # Check class distribution\n",
    "# print(df['label'].value_counts())\n",
    "\n",
    "# # Balance classes if needed\n",
    "# df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# df = df.sample(frac=1, random_state=42)\n",
    "# df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# subsample = df.sample(n=15, random_state=42)\n",
    "\n",
    "# # LABEL SENTENCES\n",
    "# # Obtain list of sentences \n",
    "# text = subsample[\"text\"]\n",
    "\n",
    "# # Testing with simple test case\n",
    "# # example = [\"hisi is cool. But I think I am cooler.\", \"www.wikipedia.com contains a lot of interesting information. John should definitely visit it.\"]\n",
    "# # text = pd.DataFrame(example, columns=['text'])\n",
    "# # text = text[\"text\"]\n",
    "\n",
    "# text_index = 0\n",
    "\n",
    "# # Iterate through each paper\n",
    "# for i in text:\n",
    "\n",
    "#     if text_index < 2:\n",
    "#         text_index += 1\n",
    "#         continue\n",
    "    \n",
    "#     # Split text into sentences\n",
    "#     sentences = sent_tokenize(i)\n",
    "\n",
    "#     # For test purposes ONLY\n",
    "#     # sentences = sentences[:100]\n",
    "\n",
    "#     # Create dataframe containing all sentences in text\n",
    "#     sentences_df = pd.DataFrame(sentences, columns=['text'])\n",
    "\n",
    "#     # Split dataframe into smaller dataframes to work with\n",
    "#     sentences_df_split = np.array_split(sentences_df,10)\n",
    "\n",
    "#     # Instantiate dataframe\n",
    "#     result = pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "\n",
    "#     # Save dataframe for current paper\n",
    "#     filename = str(text_index) + \"_sentence_labelled.csv\"\n",
    "    \n",
    "#     # Iterate through the split\n",
    "#     for split in sentences_df_split:\n",
    "#         # Manually label each sentence in the split\n",
    "#         labelled_split = get_label(split, 'text')\n",
    "\n",
    "#         # Combine labelled_split with result\n",
    "#         result = pd.concat([result, labelled_split])\n",
    "\n",
    "#         # Save current results (checkpoint)\n",
    "#         result.to_csv(filename)\n",
    "        \n",
    "#     # Manually label each sentence\n",
    "#     # sentences_df = get_label(sentences_df, 'text')\n",
    "\n",
    "#     # Save dataframe for current paper\n",
    "#     filename = str(text_index) + \"_sentence_labelled.csv\"\n",
    "#     # sentences_df.to_csv(filename)\n",
    "\n",
    "#     # Point to next paper\n",
    "#     text_index += 1\n",
    "\n",
    "#     print(\"Moving on to paper #\" + str(text_index) + \"\\n\")\n",
    "\n",
    "# print(\"Finished labelling \" + str(text_index) + \" papers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
