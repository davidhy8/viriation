{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d25255e-c52f-4199-a6f6-91602ede8514",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The purpose of this notebook is to preprocess all the BioC JSON data obtained from `pokay_processor.ipynb` so that it can be used for training of the downstream BERT models. This involves some minor regex and date filtering as well as text extraction and formating the data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ac6e84-1f2d-412f-ac6e-564c0809300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from bioc import biocjson\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a60fae1-af05-4bac-a378-5cbe532fad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions\n",
    "one_letter_aa_change = r'\\b([ARNDCQEGHILKMFPSTWYV])([1-9]+\\d*)(del|(?!\\1)[ARNDCQEGHILKMFPSTWYV])\\b'\n",
    "# three_letter_aa_change = r'\\b(?:ALA|ARG|ASN|ASP|CYS|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VAL)[1-9]+\\d*(?:ALA|ARG|ASN|ASP|CYS|DEL|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VA|DEL)\\b'\n",
    "# three_letter_aa_change = r'\\b((?:ALA|ARG|ASN|ASP|CYS|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VAL))(([1-9]+\\d*)(?!\\1)(?:ALA|ARG|ASN|ASP|CYS|DEL|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VAL)\\b'\n",
    "three_letter_aa_change = r'\\b((?:ALA|ARG|ASN|ASP|CYS|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VAL))([1-9]+\\d*)(?!(\\1))(ALA|ARG|ASN|ASP|CYS|DEL|GLN|GLU|GLY|HIS|ILE|LEU|LYS|MET|PHE|PRO|SER|THR|TRP|TYR|VAL)\\b'\n",
    "genome_change = r'\\bg\\.[ATGCU][1-9]+\\d*[ATGCU]\\b'\n",
    "genome_change_alt =  r'\\bg\\.[1-9]+\\d*[ATGCU]\\>[ATGCU]\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b5695-c5d9-47ff-b3e9-73fd05e7c202",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Load BioC JSON dictionaries from `pokay_processor.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a057df6-55c6-4ab1-b0de-5d578ebfa4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dictionary(d):\n",
    "    print(\"size: \" + str(len(d)))\n",
    "    for key in d:\n",
    "        if d[key] is None:\n",
    "            print(\"None: \" + key)\n",
    "    \n",
    "        if d[key] == \"converting\":\n",
    "            print(\"Converting: \" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a0c1e1-7c2b-4c26-98a8-54bcec5cdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load litcovid data\n",
    "# This was downloaded from https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/ \n",
    "with open('../data/pokay/litcovid2BioCJSON') as f:\n",
    "    litcovid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021c6764-d144-4e33-b452-dea961f77b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pokay data\n",
    "with open('../data/pokay/processed/data_bioc.txt') as file:\n",
    "    pokay_data = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc07ca1-737e-42a9-b36b-463c0266a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 316\n",
      "None: https://doi.org/10.1016/S1473-3099\n",
      "None: https://doi.org/10.1016/s1473-3099\n",
      "None: https://doi.org/10.1002/jmv.26997\n",
      "None: https://doi.org/10.1080/23744235.2021.1977382\n",
      "None: https://doi.org/10.1002/jmv.27247\n",
      "None: https://doi.org/10.1016/S0140-6736\n",
      "None: https://doi.org/10.1073/pnas.1707304114\n",
      "None: https://doi.org/10.21203/rs.3.rs-318392/v1\n",
      "None: https://www.researchgate.net/publication/348943694_The_mutation_P681H_in_the_B117_variant_of_SARS-CoV-2_probably_enhances_viral_entry_and_replication\n",
      "None: https://observablehq.com/@aglucaci/sc2-omicron\n",
      "None: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/961042/S1095_NERVTAG_update_note_on_B.1.1.7_severity_20210211.pdf\n",
      "None: https://doi.org/10.47326/ocsat.dashboard.2021.1.0\n",
      "None: https://www.covid19genomics.dk/2021-05-08_data-overview.html#b1525\n",
      "None: https://drive.google.com/file/d/1CuxmNYj5cpIuxWXhjjVmuDqntxXwlfXQ/view\n",
      "None: https://www.moh.gov.sg/news-highlights/details/3-new-cases-of-locally-transmitted-covid-19-infection-28apr2021-update\n",
      "None: https://mg.co.za/coronavirus-essentials/2021-03-24-single-dose-jj-janssen-covid-19-vaccine-hopes-to-speed-up-sas-vaccination-programme/\n",
      "None: https://github.com/cov-lineages/pango-designation/issues/4\n"
     ]
    }
   ],
   "source": [
    "check_dictionary(pokay_data) # Check over pokay_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9219bdb-acfb-4546-aabc-c4794370db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data_keys\n",
    "with open(\"../data/pokay/data_keys.txt\") as file:\n",
    "    pokay_keys = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6eb0786-8080-4b13-af19-dd90955d5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dictionary(pokay_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401de86-0735-42a7-ae76-f6cecdb28997",
   "metadata": {},
   "source": [
    "# Basic filters\n",
    "Perform basic filtering on BioC JSON from litcovid database. This will be used as negative examples in downstream training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559315e-4919-4971-af5b-2d522c59814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using Regex to identify mutations\n",
    "\n",
    "count = 0\n",
    "filtered_papers = []\n",
    "\n",
    "for paper in litcovid_data[1]:\n",
    "    \n",
    "    try:\n",
    "        passage = paper[\"passages\"]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    text = \"\"\n",
    "\n",
    "    for section in passage:\n",
    "        # print(\" \")\n",
    "        # print(section) \n",
    "        try:\n",
    "            text += section['text']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    mutations = []\n",
    "    mutations += [\"\".join(x) for x in re.findall(one_letter_aa_change, text, re.IGNORECASE)]\n",
    "    mutations += [\"\".join(x) for x in re.findall(three_letter_aa_change, text, re.IGNORECASE)]\n",
    "    mutations += re.findall(genome_change, text, re.IGNORECASE)\n",
    "    mutations += re.findall(genome_change_alt, text, re.IGNORECASE)\n",
    "    mutations = set(mutations)\n",
    "\n",
    "    if len(mutations) > 0:\n",
    "        filtered_papers.append(paper)\n",
    "\n",
    "with open('../data/pokay/filtered_papers.txt', 'w') as file:\n",
    "     file.write(json.dumps(filtered_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "317c23b9-c62b-4703-b5d9-58b6e962539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered papers\n",
    "with open('../data/pokay/filtered_papers.txt') as file:\n",
    "        filtered_papers = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edb5b999-738e-4e67-a37e-941a992ebafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove papers that are in pokay database\n",
    "\n",
    "def related_paper(paper):\n",
    "    try:\n",
    "        doi = paper[\"passages\"][0]['infons']['article-id_doi']\n",
    "        \n",
    "        if doi in pokay_data:\n",
    "            return True\n",
    "            \n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    return False\n",
    "\n",
    "filtered_papers_copy = [x for x in filtered_papers if not related_paper(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ec9620-569d-4346-8111-0eef3b55dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date. Only grab papers before 2021\n",
    "\n",
    "before_date_filtered_papers = []\n",
    "\n",
    "for paper in filtered_papers_copy:  \n",
    "    try:\n",
    "        year = paper[\"year\"]\n",
    "        if int(year) <= 2021:\n",
    "            before_date_filtered_papers.append(paper)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d55800a-a956-4550-8213-16ad6f582294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date. Only grab papers after 2021\n",
    "\n",
    "after_date_filtered_papers = []\n",
    "\n",
    "for paper in filtered_papers_copy:  \n",
    "    try:\n",
    "        year = paper[\"year\"]\n",
    "        if int(year) > 2021:\n",
    "            after_date_filtered_papers.append(paper)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a00b3-f974-48d3-8c33-46c6b48d20d6",
   "metadata": {},
   "source": [
    "# Break into subtasks\n",
    "Helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55ec4eb1-a6bd-47cb-90fc-9814fb7121d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab subsample from data\n",
    "def subset_sample(original, n):\n",
    "    sub = []\n",
    "    df = copy.deepcopy(original)\n",
    "    random.seed(42)\n",
    "    random.shuffle(df)\n",
    "    \n",
    "    for i in range(n):\n",
    "        entry = df.pop(-1)\n",
    "        sub.append(entry)\n",
    "\n",
    "    return df, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5227e95e-d235-410e-8992-2bbfc2a31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to un-nest data. Example JSON file will contain Key1: {Key 2: {Key3: Val}}  \n",
    "def extract_nested_elements(input_string):\n",
    "    elements = []\n",
    "    start = 0\n",
    "    brace_count = 0\n",
    "    inside_element = False\n",
    "\n",
    "    for i, char in enumerate(input_string):\n",
    "        if char == '{':\n",
    "            if brace_count == 0:\n",
    "                start = i\n",
    "                inside_element = True\n",
    "            brace_count += 1\n",
    "        elif char == '}':\n",
    "            brace_count -= 1\n",
    "            if brace_count == 0 and inside_element:\n",
    "                elements.append(input_string[start:i+1])\n",
    "                inside_element = False\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a07286b8-dff4-4fc0-bd3b-e84f48feceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from training data (litcovid portion), passes DOI, output is dictionary \n",
    "def litcovid_text_extract(data):\n",
    "    count = 0\n",
    "    out = {}\n",
    "    for paper in data:\n",
    "        try:\n",
    "            passage = paper[\"passages\"]\n",
    "            pmid = paper[\"pmid\"]\n",
    "        except:\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        text = \"\"\n",
    "        \n",
    "        for section in passage:\n",
    "            try:\n",
    "                text += section['text']\n",
    "        \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if text[-1].isalnum(): \n",
    "                text += \". \"\n",
    "            else:\n",
    "                text += \" \"\n",
    "\n",
    "        out[pmid] = text\n",
    "        \n",
    "    # print(count)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3986eaca-591e-4101-8523-e1449b400c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab file name from DOI\n",
    "def get_file_name(key):\n",
    "    doi_pattern = r'https:\\/\\/doi\\.org\\/[\\w/.-]+'\n",
    "    doi = re.search(doi_pattern, key)\n",
    "\n",
    "    if doi is not None:\n",
    "        file_name = key.split('doi.org/')[-1]\n",
    "    else:\n",
    "        key = key.split('https://')[-1]\n",
    "        file_name = key\n",
    "\n",
    "    # Replace . in DOI with -\n",
    "    file_name = file_name.replace(\".\", \"-\")\n",
    "    # Replace / in DOI with _\n",
    "    file_name = file_name.replace(\"/\", \"_\")\n",
    "    # file_name += \".pdf\"\n",
    "\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12636ac3-e851-4338-b06e-610a76512f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text extract of JSON from pubtator API\n",
    "def pubtator_extract(paper):\n",
    "    text = \"\"\n",
    "    paper = paper[1:-1]\n",
    "\n",
    "    try:\n",
    "        bioc_list = extract_nested_elements(paper)\n",
    "        \n",
    "        bioc_collection = biocjson.loads(bioc_list[-1])\n",
    "        \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    for document in bioc_collection.documents:    \n",
    "        for passage in document.passages:\n",
    "            try:\n",
    "                text += passage.text\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if text[-1].isalnum(): \n",
    "                text += \". \"\n",
    "            else:\n",
    "                text += \" \"\n",
    "   \n",
    "    if text == \"\":\n",
    "        return None\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af73e7be-0286-461a-a0b3-2c54745a5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text extract of JSON from conversions of JATS XML\n",
    "def jats_extract(paper):\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        paper_copy = paper[1:-1]\n",
    "        bioc_collection = biocjson.loads(paper_copy)\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            bioc_collection = biocjson.loads(paper)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    for document in bioc_collection.documents:    \n",
    "        for passage in document.passages:\n",
    "            try:\n",
    "                text += passage.text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if text[-1].isalnum(): \n",
    "                text += \". \"\n",
    "            else:\n",
    "                text += \" \"\n",
    "\n",
    "    if text == \"\":\n",
    "        return None\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980563a8-b7b9-4251-a98a-ad806a68f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text extract of JSON from conversions of PDF\n",
    "def pdf_extract(data):\n",
    "    text = \"\"\n",
    "\n",
    "    try:\n",
    "        bioc_collection = biocjson.loads(paper)\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    for document in bioc_collection.documents:    \n",
    "        for passage in document.passages:\n",
    "            try:\n",
    "                text += passage.text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if text[-1].isalnum(): \n",
    "                text += \". \"\n",
    "            else:\n",
    "                text += \" \"\n",
    "\n",
    "    if text == \"\":\n",
    "        return None\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40fce905-1eec-4e29-9c2a-115f5d4bc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pokay text from each individual pokay paper\n",
    "def pokay_text_extract(paper):\n",
    "    text_extracted = False\n",
    "    text = \"\"\n",
    "    \n",
    "    if paper is not None:\n",
    "        # Try to extract as pubtator\n",
    "        try:\n",
    "            text = pubtator_extract(paper)\n",
    "\n",
    "            if text is not None:\n",
    "                text_extracted = True\n",
    "                pokay_text.append(text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if text_extracted:\n",
    "            return text\n",
    "\n",
    "        # Try to extract as JATS\n",
    "        try:\n",
    "            text = jats_extract(paper)\n",
    "\n",
    "            if text is not None:\n",
    "                text_extracted = True\n",
    "                pokay_text.append(text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if text_extracted:\n",
    "            return text\n",
    "\n",
    "        # Try to extract as PDF\n",
    "        try:\n",
    "            text = pdf_extract(paper)\n",
    "\n",
    "            if text is not None:\n",
    "                text_extracted = True\n",
    "                pokay_text.append(text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        file = get_file_name(key)\n",
    "        file = \"../data/raw/pdf/unconverted/\" + file + \".pdf\"\n",
    "        isExist = os.path.exists(file) \n",
    "        if isExist:\n",
    "            print(file)\n",
    "            reader = pypdf.PdfReader(file)\n",
    "    \n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1983a6ee-5054-4f88-9a5d-3a515b461fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function to handle all cases for pokay data\n",
    "def pokay_extract(data):\n",
    "    pokay_text = []\n",
    "\n",
    "    for key in data:\n",
    "        paper = data[key]\n",
    "        text_extracted = False\n",
    "        text = \"\"\n",
    "        \n",
    "        if paper is not None:\n",
    "            # Try to extract as pubtator\n",
    "            try:\n",
    "                text = pubtator_extract(paper)\n",
    "    \n",
    "                if text is not None:\n",
    "                    text_extracted = True\n",
    "                    pokay_text.append(text)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            if text_extracted:\n",
    "                continue\n",
    "    \n",
    "            # Try to extract as JATS\n",
    "            try:\n",
    "                text = jats_extract(paper)\n",
    "    \n",
    "                if text is not None:\n",
    "                    text_extracted = True\n",
    "                    pokay_text.append(text)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            if text_extracted:\n",
    "                continue\n",
    "    \n",
    "            # Try to extract as PDF\n",
    "            try:\n",
    "                text = pdf_extract(paper)\n",
    "    \n",
    "                if text is not None:\n",
    "                    text_extracted = True\n",
    "                    pokay_text.append(text)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        else:\n",
    "            file = get_file_name(key)\n",
    "            file = \"../data/raw/pdf/unconverted/\" + file + \".pdf\"\n",
    "            isExist = os.path.exists(file) \n",
    "            if isExist:\n",
    "                print(file)\n",
    "                reader = pypdf.PdfReader(file)\n",
    "        \n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "        \n",
    "                if text != \"\":\n",
    "                    pokay_text.append(text)\n",
    "    \n",
    "    return pokay_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375b9fa-3045-45d2-8f5d-dd6bc940d0ef",
   "metadata": {},
   "source": [
    "# Create datasets for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16b9934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset for initial training\n",
    "litcovid, train_data = subset_sample(before_date_filtered_papers, 309) # Subsample negative examples for training data\n",
    "train_data_text = litcovid_text_extract(train_data) # Extract text from BioC JSON\n",
    "pokay_text = pokay_extract(pokay_data) # Extract text from BioC JSON of Pokay (positive examples)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(train_data_text, columns=[\"text\"])\n",
    "df[\"label\"] = 0\n",
    "\n",
    "df_2 = pd.DataFrame(pokay_text, columns=[\"text\"])\n",
    "df_2[\"label\"] = 1\n",
    "\n",
    "df = pd.concat([df, df_2])\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"../data/pipeline_data/paper_flagging_data/bert_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c3b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retraining dataset\n",
    "papers, retrain_data = subset_sample(litcovid, 500) # \n",
    "retrain_data_text = litcovid_text_extract(retrain_data)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(retrain_data_text, columns=[\"text\"])\n",
    "\n",
    "# Save chunks dataset -> this dataset will need to be manually annotated using the notebook dataset_labeller.ipynb\n",
    "df.to_csv('../data/pipeline_data/paper_flagging_data/chunks_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae53d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with 500 papers after 2021 (example of data we scrape in the future)\n",
    "new_papers, data = subset_sample(after_date_filtered_papers, 500)\n",
    "data_text = litcovid_text_extract(data)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(data_text, columns=[\"text\"])\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"../data/pipeline_data/paper_flagging_data/new_papers_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb26b4e-6a95-4184-a7b3-e4d1c1ede88b",
   "metadata": {},
   "source": [
    "# Create dataset for BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c31d0c2f-5ed3-40f1-858f-e189416c99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pokay_keys = {key: \"\\n\".join(value) for key, value in pokay_keys.items()}\n",
    "\n",
    "# Input text\n",
    "df = pd.DataFrame.from_dict(pokay_data, orient='index', columns=[\"text\"])\n",
    "\n",
    "# Summary output\n",
    "df_2 = pd.DataFrame.from_dict(combined_pokay_keys, orient='index', columns=['summary'])\n",
    "\n",
    "pokay = pd.merge(df, df_2, left_index=True, right_index=True)\n",
    "pokay = pokay.dropna()\n",
    "\n",
    "pokay[\"text\"] = pokay[\"text\"].map(pokay_text_extract)\n",
    "\n",
    "pokay.to_csv('../data/pipeline_data/paper_flagging_data/BART_dataset.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
