{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8864502b-0fe6-45a9-9ca7-73fb48cd4c8f",
   "metadata": {},
   "source": [
    "# Tutorial for adapting viriation to another virus of choice\n",
    "This tutorial will guide you through adapting the viriation program for any virus of your choice. The process primarily involves fine-tuning BERT and gradient boosting models to suit your specific needs.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [**Installation**](#Installation)\n",
    "2. [**Re-training BERT model**](#BERT)\n",
    "3. [**Re-training Gradient Boosting model**](#LightGBM)\n",
    "4. [**Refactoring program**](#Refactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71049c5-5e5e-4787-997d-2be001c3ee0a",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Follow steps 1, 2, 3, and 5 of the [installation guide](https://github.com/davidhy8/viriation/wiki/Installation-Guide) to setup the program. This will involve setting up your conda and pip environments and directories. In your case, you will also need to create a separate directory train at `data/train` so that you can save your training data for your models. You may use the following commands:\n",
    "\n",
    "```\n",
    "cd data/\n",
    "mkdir -p train/raw/ train/processed/bioc/ train/processed/html/\n",
    "```\n",
    "\n",
    "These folder directories can serve as intermediary locations to save information while trying to prepare your training data. Below a script consisting of helper functions that are needed later is loaded for convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d39904a-942e-4f65-945f-3c87bd341629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NCBI_API_KEY=\"6667a919224612da1287d74ff0d3f7b5e208\"\n",
      "No record can be found for the input: 9005165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 09:23:54 cn0660 numexpr.utils[3611410] INFO Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-09-23 09:23:54 cn0660 numexpr.utils[3611410] INFO NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "%run helper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e25c7-cf30-4e00-b319-bc3fd589a148",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "The current PubMedBERT model is specifically trained to identify text chunks discussing SARS-CoV-2 mutations, so it will not flag texts reporting mutations from other viruses. To adapt the BERT model for your virus of interest, we recommend following the same procedure we used to train our PubMedBERT model, as outlined in `notebook/train.ipynb`. You can find detailed instructions [here](https://github.com/davidhy8/viriation/wiki/Model-Training-for-COVID%E2%80%9019).\n",
    "\n",
    "If a database of mutations is not available for your virus of interest, you may need to manually prepare data to train the BERT model. In our study on SARS-CoV-2, we used 300 positive and 300 negative examples of literature (publications, preprints, and grey literature) reporting SARS-CoV-2 mutations. We recommend preparing a similar amount of data for your virus if possible. If very few relevant publications are available for your virus, it may be useful to use our pre-trained SARS-CoV-2 model as a base model and fine-tune it with the limited data you have.\n",
    "\n",
    "The second step in training the BERT model involves fine-tuning it with annotated text chunks, each containing 512 tokens. This process of data annotation is made easy with the notebook `notebook/dataset_labeller.ipynb`, which reads a dataframe containing columns for DOI and text (the full document) and splits the document into chunks for labeling in the command line. In the SARS-CoV-2 study, approximately 100 text chunks were annotate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b3964-939e-43d5-a31f-d5eace0b893a",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "\n",
    "In order to fine-tune the BERT model for your specific needs, you must find **positive** and **negative examples** of literature reporting novel mutations from your virus of choice. Your collection of papers may include publications, preprints and grey literature, but they should categorized accordingly to ease BioC JSON retrieval in the latter steps. For example you can gather the DOIs of your papers in JSON format as follows:\n",
    "\n",
    "`{'DOI_1': 'Preprint', 'DOI_2: 'Publication', 'DOI_3': 'Grey literature', ....}`\n",
    "\n",
    "`notebook/pokay_processor.ipynb` and `notebook/full_preprocessing.ipynb` demonstrates the process performed for preparing the training data for SARS-CoV-2. In the segment below we provide template code for processing your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561e9f98-a33f-45b0-9da1-4e5dc7b06769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/train/raw/papers.json') as json_file: # Read in JSON file with collection of papers\n",
    "    papers = json.load(json_file)\n",
    "\n",
    "# Split dictionary by literature type\n",
    "publications_bioc = {}\n",
    "preprint_bioc = {}\n",
    "grey_bioc = {}\n",
    "\n",
    "for key, value in papers.items():\n",
    "    if value == 'publication':\n",
    "        publications_bioc[key] = None\n",
    "    elif value == 'preprint':\n",
    "        preprint_bioc[key] = None\n",
    "    elif value == 'grey':\n",
    "        grey_bioc[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e14929-f984-4b00-93a1-6e1289e762de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve BioC JSON file for each type of literature\n",
    "publication_bioc, publication_unk_bioc = get_journal_publication_bioc(publication_bioc) # publications\n",
    "rxiv_bioc, rxiv_unk_bioc = get_rxiv_bioc(preprint_bioc) # preprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5e039-2d06-4c0e-86a0-228c59d8b455",
   "metadata": {},
   "source": [
    "For grey literature, these are usually only available as PDFs and undergo a series of conversions in order to change into BioC JSON format. You may use the following steps:\n",
    "1. Download PDF version of article\n",
    "2. Convert the file to HTML format using Allen Institute's [tool](https://papertohtml.org)\n",
    "3. Using AutoCorpus, convert the HTML file to BioC JSON format:\n",
    "```\n",
    "cd submodules/autocorpus/\n",
    "# python run_app.py -c \"../../data/other/config_allen.json\" -t \"../../data/train/processed/bioc/\" -f \"../../data/train/processed/html/\" -o JSON\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715292a0-b99d-4dbc-be33-e6efbe8a4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grey literature from directory\n",
    "for key in grey_bioc:\n",
    "    file = get_file_name(key)\n",
    "    file = \"~/viriation/data/train/processed/bioc/\" + file + \"_bioc.json\" # might have to adjust location\n",
    "    try:\n",
    "        grey_bioc[key] = Path(file).read_text().replace('\\n', '')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb45f4-7e5c-4f5c-82fe-659b28f3eb5f",
   "metadata": {},
   "source": [
    "After retreiving the BioC JSON format for all literature, now we will extract the text from each JSON file and format it into a dataframe for model training. Recall that in addition to a set of positive examples, we also need a set of negative examples for the data which can be prepared in similar fashion as the instructions above. Note that the amount of positive and negative examples should be exactly equal so that the training data doesn't skew the model towards classifying in one  direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7b547-bf70-4553-9ccc-ab1fb63fc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = publication_bioc.copy()\n",
    "pos = pos.update(rxiv_bioc)\n",
    "pos = pos.update(grey_bioc) # merge dictionaries to get positive examples\n",
    "\n",
    "neg = negative_examples # separately prepare negative examples with exact same number of papers as positive example\n",
    "\n",
    "pos_text = litcovid_text_extract(pos) # Extract text from BioC JSON of positive examples\n",
    "neg_text = pokay_extract(neg) # Extract text from BioC JSON of negative examples\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(neg_text, columns=[\"text\"])\n",
    "df[\"label\"] = 0\n",
    "\n",
    "df_2 = pd.DataFrame(pos_text, columns=[\"text\"])\n",
    "df_2[\"label\"] = 1\n",
    "\n",
    "df = pd.concat([df, df_2])\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(\"../data/train/processed/bert_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413c1ce-7259-463a-b92e-6cd374a325f4",
   "metadata": {},
   "source": [
    "The dataset provided is used for the initial round of training. To re-train the model, a new dataset is required. In this second dataset, each of the 512 text chunks must be manually labeled, making this process very time-consuming, and therefore, it is only done for a small subset of text chunks - text chunks that our initial model cannot predict with high confidence. While it might seem redundant to label and train on these text chunks now, instead of doing it in the first round, labeling all the text chunks for a large number of papers would have been too time-intensive and retraining the model along it's decision boundary is also very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253092b6-9187-4afa-a258-d96c04e3416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retraining dataset\n",
    "retrain_data  # Load retrain data (preprocess in the same way as the positive examples) \n",
    "retrain_data_text = litcovid_text_extract(retrain_data) # Extract text from JSON\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(retrain_data_text, columns=[\"text\"])\n",
    "\n",
    "# Save chunks dataset -> this dataset will need to be manually annotated\n",
    "df.to_csv('../data/train/processed/chunks_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e26e6-07f7-4a0c-ada7-31e06b0a0c90",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "Once the training data is prepared, we will train our BERT model in two stages, using the datasets specifically created for each round. This can be done very easily by swapping the training data (`bert_dataset.csv` & `chunks_dataset.csv`) specified in `/notebook/train.ipynb` with the datasets above and modifying the input/output paths. The rest of the training process is covered thoroughly in the notebook. \n",
    "\n",
    "During the model re-training step, you will have to label the retrain_df_1 which consists of text chunks that the model struggles to predict. You may use the code and labelling guidelines below to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c11c28-3725-4b21-9047-a4ab53e15a34",
   "metadata": {},
   "source": [
    "**Labelling guidelines**, only label 1 (Yes) if it:\n",
    "- Includes specific terms like \"Delta variant\" or any specific variant name\n",
    "- Includes terms like \"mutation\", \"viral variant\", \"strain\", \"variant of concern\" and \"genetic variant\"\n",
    "- Describes characteristics, behaviours, or impacts of the variants, even if not named explicitly\n",
    "- Compares different variants\n",
    "- Discusses genetic mutations or sequences related to viral variants\n",
    "- Discusses spread, transmission rates or infection rates associated with specific variants\n",
    "- Discusses how variants affect vaccine efficacy\n",
    "- Describes changes in symptom severity due to variants\n",
    "- Discusses health outcomes associated with variants\n",
    "- Discusses public health measures or responses tailored to specific variants\n",
    "- Discusses time and place of variant emergence and spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2643080-314a-4ca6-bd31-4dbea0cb0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL CHUNKS\n",
    "# Obtain list of sentences \n",
    "df = pd.read_csv(\"../data/train/processed/retrain_df_1.csv\")\n",
    "# text = df.sample(n=3)\n",
    "\n",
    "# Split dataframe into smaller dataframes to work with\n",
    "df_split = np.array_split(df,30)\n",
    "\n",
    "# Instantiate dataframe\n",
    "result = pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "\n",
    "# Iterate through each split\n",
    "for i, split in enumerate(df_split):\n",
    "    # if i < 3:\n",
    "    #     continue\n",
    "    print(\"Current split \" + str(i))\n",
    "    labelled_chunks = get_label(split, 'text')\n",
    "    filename = \"../data/train/processed/\" + str(i) + \"_chunks_labelled.csv\"\n",
    "    labelled_chunks.to_csv(filename)\n",
    "    # labelled_chunks[\"position\"] = i\n",
    "    labelled_chunks.to_csv(filename)\n",
    "    print(\"Finished split \" + str(i))\n",
    "    print(\"============================================================\")\n",
    "    # Combine labelled_split with result\n",
    "    result = pd.concat([result, labelled_chunks])\n",
    "    \n",
    "result.to_csv(\"../data/train/processed/labelled_chunks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429090bb-f25f-45ff-b5fa-c42c81da112e",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "The gradient boosting model was originally trained to assess the positional importance of each text chunk within a paper when predicting the paper's overall relevance. Since positional information in literature (e.g., abstracts, conclusions) typically remains consistent, fine-tuning the gradient boosting model is optional. If you choose to fine-tune it, refer to `notebook/lightGBM_train.ipynb` for guidance. Note that the original model was trained using over 10,000 text chunks from 600 papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce9625-2f4f-4151-9c89-769e2e1eb810",
   "metadata": {},
   "source": [
    "# Refactor\n",
    "\n",
    "The web-scraping program was originally designed to capture new literature on SARS-CoV-2. To collect papers discussing your virus of interest, you will need to update the search terms in `run.sh` and `app/scripts/scrape_papers.py`.\n",
    "Currently, the search terms used for literature scraping are for COVID-19. For your virus of choice, you will need to modify these terms and implement it into the following spots in the program. \n",
    "\n",
    "1. On line 145 of `app/scripts/scrape_papers.py`, modify `covid_terms = ['coronavirus', 'ncov', 'cov', '2019-nCoV', 'SARS-CoV-2', 'COVID19', 'COVID']` with your own terms.\n",
    "\n",
    "2. On line 27 of `run.sh`, modify `esearch -db pubmed -query \"('coronavirus'[All Fields] OR 'ncov'[All Fields] OR 'cov'[All Fields] OR '2019-nCoV'[All Fields] OR 'SARS-CoV-2'[All Fields] OR 'COVID19'[All Fields] OR 'COVID-19'[All Fields] OR 'COVID'[All Fields]) AND (\\\"${start_pm}\\\"[CRDT] : \\\"${end_pm}\\\"[CRDT]) NOT preprint[pt]\" | efetch -format docsum > data/scraper/pubmed/litcovid.xml` with your own terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b7e19-a7c2-4224-8be8-a0d357023828",
   "metadata": {},
   "source": [
    "Example with search terms \"HIV\", and \"HIV-A\":\n",
    "\n",
    "```\n",
    "esearch -db pubmed -query \"('HIV'[All Fields] OR 'HIV-A'[All Fields]) AND (\\\"${start_pm}\\\"[CRDT] : \\\"${end_pm}\\\"[CRDT]) NOT preprint[pt]\" | efetch -format docsum > data/scraper/pubmed/litcovid.xml\n",
    "``` \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
