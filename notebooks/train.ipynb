{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f96cca-d500-4172-b557-7aaea2bc30b5",
   "metadata": {},
   "source": [
    "# Model training process\n",
    "Fine-tuning the PubmedBERT model through two stages of training then preparing dataset for training the gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe099315-a725-415e-b9fd-0ad9e324e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/david.yang1/.cache/huggingface/'\n",
    "os.environ['HF_HOME'] = '/home/david.yang1/.cache/huggingface/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2daaaf9-b7aa-4b88-8731-117600c28586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-07-18 09:51:03.705982: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 09:51:03.749374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 09:51:03.749412: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 09:51:03.749448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 09:51:03.758072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 09:51:04.632573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "# from huggingface_hub import login\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "# from ray.tune.search.hyperopt import HyperOptSearch\n",
    "# from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f13ca8-54a9-4eed-b20c-fc056367b5b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phase 1 of training BERT model\n",
    "Training base PubmedBERT model with Pokay full-text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438d10-ed93-4005-8110-84f9a48c39fb",
   "metadata": {},
   "source": [
    "## Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936dd18c-ad8f-4fa3-8971-cf2ec58a8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(\n",
    "        df['text'],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df581e6-362f-4698-8022-ce12b4e66de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for models\n",
    "def ds_preparation(df, val_count=0):\n",
    "    # Balance classes if needed\n",
    "    df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = df[[\"text\", \"label\"]]\n",
    "    \n",
    "    # Split dataset into test & train\n",
    "    df_train = df[val_count:]\n",
    "    df_val = df[:val_count]\n",
    "    \n",
    "    tds = Dataset.from_pandas(df_train)\n",
    "    vds = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    tds = tds.map(tokenize_function, batched=True)\n",
    "    vds = vds.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format of the datasets to include only the required columns\n",
    "    tds = tds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    vds = vds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    \n",
    "    # Define DatasetDict\n",
    "    ds = DatasetDict({\n",
    "        \"train\": tds,\n",
    "        \"validation\": vds\n",
    "    })\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07a8d-89f1-4c65-96ca-f649ecb71544",
   "metadata": {},
   "source": [
    "## Model fine tuning\n",
    "\n",
    "Fine-tuning model with annotated text chunk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4991a887-017e-42c9-9348-00fb299443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38cf31b-9a57-43aa-a71a-a1026c6ddeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = False\n",
    "\n",
    "# Load initial fine-tuned model\n",
    "if checkpoint:\n",
    "    def model_init():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\"./chunks-pubmed-bert\", num_labels=2)\n",
    "else:\n",
    "    def model_init():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\"NeuML/pubmedbert-base-embeddings\", num_labels=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5b9950-b36c-47f0-bf4e-74cfaf319ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model based off given dataset\n",
    "def fine_tune_model(ds, model_init, train=False):\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=3,    # number of training epochs\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Create the Trainer\n",
    "    trainer = Trainer(\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        model_init=model_init,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        trainer.train()\n",
    "\n",
    "    if ds[\"validation\"]:\n",
    "        trainer.evaluate()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86482e9f-cee7-421f-b90b-e32dc0be10fb",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "Functions to perform text chunk predictions & model performance evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e53981-bcdc-4cec-bbf9-90fac21d6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into <512 token chunks\n",
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, overlap_sentences=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence using BERT Tokenizer\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # Finalize the current chunk if adding this sentence exceed token limit\n",
    "        if current_chunk_len + token_count > max_tokens:\n",
    "            text_chunk = \"\".join(current_chunk)\n",
    "            chunks.append(text_chunk)\n",
    "\n",
    "            # Create the next chunk with overlap\n",
    "            overlap_start = max(0, i-overlap_sentences)\n",
    "            current_chunk = []\n",
    "            for j in range(overlap_start, i):\n",
    "                current_chunk.append(sentences[j])\n",
    "            current_chunk_len = len(current_chunk)\n",
    "\n",
    "        # Add the current sentence tokens to the chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_len += token_count\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        text_chunk = \"\".join(current_chunk)\n",
    "        chunks.append(text_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9793bb-d2bc-47b7-8a8c-2a5f28a38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label of dataframe\n",
    "def prediction_chunks(df, tokenizer, trainer):\n",
    "    output = pd.DataFrame()\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "        chunks = split_text_into_chunks(text, tokenizer)\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "        # chunks_df[\"label\"] = df[\"label\"][i]\n",
    "        chunks_df[\"position\"] = chunks_df.index\n",
    "        chunks_df[\"paper\"] = i\n",
    "        \n",
    "        t = Dataset.from_pandas(chunks_df)\n",
    "        t = t.map(tokenize_function, batched=True)\n",
    "        ds_t = DatasetDict({\n",
    "            \"test\": t\n",
    "        })\n",
    "\n",
    "        pred = trainer.predict(ds_t[\"test\"])\n",
    "        chunks_df[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "\n",
    "        # convert logit score to torch array\n",
    "        torch_logits = torch.from_numpy(pred.predictions)\n",
    "\n",
    "        # get probabilities using softmax from logit score and convert it to numpy array\n",
    "        probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()\n",
    "\n",
    "        chunks_df[\"probability\"] = probabilities_scores.max(-1)\n",
    "\n",
    "        # save into output\n",
    "        output = pd.concat([output, chunks_df], ignore_index=True)\n",
    "        \n",
    "    return output, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22be07ee-b6e7-4ddf-b3f4-0025c5b3c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8641f7934b4c33bae9088d133ca25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load prediction chunks\n",
    "pred_chunks_0 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/0_chunks_labelled.csv\")\n",
    "pred_chunks_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/1_chunks_labelled.csv\")\n",
    "pred_chunks_2 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/2_chunks_labelled.csv\")\n",
    "pred_chunks_3 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/3_chunks_labelled.csv\")\n",
    "pred_chunks_4 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/4_chunks_labelled.csv\")\n",
    "pred_chunks_5 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/5_chunks_labelled.csv\")\n",
    "pred_chunks_6 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/6_chunks_labelled.csv\")\n",
    "pred_chunks_7 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/7_chunks_labelled.csv\")\n",
    "pred_chunks_8 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/8_chunks_labelled.csv\")\n",
    "pred_chunks_9 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/9_chunks_labelled.csv\")\n",
    "pred_chunks_10 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/10_chunks_labelled.csv\")\n",
    "pred_chunks_11 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/11_chunks_labelled.csv\")\n",
    "\n",
    "\n",
    "# Concatenate data\n",
    "df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2, pred_chunks_3, pred_chunks_4, pred_chunks_5, pred_chunks_6, pred_chunks_7, \n",
    "                     pred_chunks_8, pred_chunks_9, pred_chunks_10, pred_chunks_11])\n",
    "\n",
    "# Load dataframe as dataset\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenize test dataset\n",
    "test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format of the datasets to include only the required columns\n",
    "test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds_test = DatasetDict({\n",
    "    \"test\": test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35f5b7e-001a-4467-87ff-a6a16a96d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for chunk based prediction\n",
    "def validate_model(trainer):\n",
    "    # Test performance of the model on labeled chunks\n",
    "    pred = trainer.predict(ds_test[\"test\"])\n",
    "    \n",
    "    df_test[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "    \n",
    "    metrics = compute_metrics(pred)\n",
    "\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed027-9924-4660-8ad7-c3fa05bc1085",
   "metadata": {},
   "source": [
    "## Perform prediction and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8e91e-94ca-4fd0-a447-139ffa0ef7c0",
   "metadata": {},
   "source": [
    "**Documentation**: Transfer learning of PubmedBERT with Pokay dataset at ./chunks-pubmed-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2987bc2d-2ef7-48ac-a56c-262491053963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49673a8eaf6e4126806c3f4b7a9d446d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9cf597422431a8dd4ffa40de07bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NeuML/pubmedbert-base-embeddings and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NeuML/pubmedbert-base-embeddings and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 14:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.656, 'f1': 0.6666666666666665, 'precision': 0.5119047619047619, 'recall': 0.9555555555555556}\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"NeuML/pubmedbert-base-embeddings\", num_labels=2)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/pipeline_data/paper_flagging_data/bert_dataset.csv')\n",
    "ds = ds_preparation(df, val_count=128)\n",
    "\n",
    "# Train model\n",
    "trainer = fine_tune_model(ds, model_init, train=True)\n",
    "\n",
    "# Checkpoint model\n",
    "trainer.save_model(\"../models/chunks-pubmed-bert\")\n",
    "\n",
    "# Validate current model\n",
    "metrics = validate_model(trainer)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f3765-1b52-4889-a3b1-12325d019e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "# Check performance\n",
    "metrics = validate_model(trainer)\n",
    "print(metrics)\n",
    "# Save predictions \n",
    "df_test.to_csv(\"../data/pipeline_data/paper_flagging_data/test_predictions_model_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2352c0-94a3-4cb5-990e-9975d3bb9519",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phase 2 of training BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed4ad-62a9-4ae6-acc9-f40710f3e08b",
   "metadata": {},
   "source": [
    "## Load retrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c96610-0845-4444-820b-69c9382d4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retrain data\n",
    "retrain_df = pd.read_csv('../data/pipeline_data/paper_flagging_data/chunks_dataset.csv')\n",
    "retrain_df = retrain_df.sample(frac=1, random_state=42)\n",
    "\n",
    "retrain_df_1 = retrain_df[:100]\n",
    "retrain_df_1, _ = prediction_chunks(retrain_df_1, tokenizer, trainer)\n",
    "# retrain_df_1\n",
    "\n",
    "retrain_df_1.describe()\n",
    "\n",
    "# Rename column\n",
    "retrain_df_1 = retrain_df_1.rename(columns={'prediction': 'label'})\n",
    "\n",
    "print(retrain_df_1[retrain_df_1[\"probability\"] < 0.7].count())\n",
    "\n",
    "# Filter for predictions by the decision boundary\n",
    "retrain_df_1 = retrain_df_1[retrain_df_1[\"probability\"] < 0.7]\n",
    "\n",
    "# Balance out 0 and 1 labels \n",
    "retrain_df_1 = retrain_df_1.groupby('label').sample(n=min(retrain_df_1['label'].value_counts()), random_state=42)\n",
    "\n",
    "# Save dataset locally to verify validity of predictions\n",
    "retrain_df_1.to_csv(\"../data/pipeline_data/paper_flagging_data/retrain_df_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df69d1-4c6b-409e-9602-2d623c7233c8",
   "metadata": {},
   "source": [
    "Verify the integrity of dataset manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fdc06-29e0-4584-ae05-7deeb61f27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modified dataset\n",
    "retrain_df_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/retrain_df_1.csv\")\n",
    "\n",
    "# remove german paper\n",
    "retrain_df_1 = retrain_df_1.drop(retrain_df_1[retrain_df_1['paper'] == 43].index)\n",
    "\n",
    "# balance out classes\n",
    "retrain_df_1 = retrain_df_1.groupby('label').sample(n=min(retrain_df_1['label'].value_counts()), random_state=42)\n",
    "\n",
    "# Drop unneccessary columns\n",
    "retrain_df = retrain_df_1.drop(['position', 'paper', 'probability'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d1f68-d8d5-471d-b5d0-09f0d0841b88",
   "metadata": {},
   "source": [
    "**Documentation**: June 28 - Retrain of model #1 saved at ./chunks-pubmed-bert-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e54ed-d24b-4faa-b435-3f36aac3f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "retrain_ds = ds_preparation(retrain_df)\n",
    "\n",
    "# Load model \n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert\", num_labels=2)\n",
    "\n",
    "# Train model\n",
    "trainer = fine_tune_model(retrain_ds, model_init, train=True)\n",
    "\n",
    "# Checkpoint model\n",
    "trainer.save_model(\"../models/chunks-pubmed-bert-v2\")\n",
    "\n",
    "# Validate current model\n",
    "metrics = validate_model(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054dd616-10db-4bc6-8f78-9e0906c99dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create training dataset for gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a7ac2-9935-410f-9f2f-a8358ae43034",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_data = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/bert_dataset.csv\")\n",
    "\n",
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../models/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "chunked_data = prediction_chunks(full_text_data, tokenizer, trainer) # prediction of text chunks from BERT model\n",
    "\n",
    "chunked_data_df, chunked_data_pred = chunked_data\n",
    "\n",
    "chunked_data_df.to_csv(\"../data/pipeline_data/paper_flagging_data/lightgbm.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
